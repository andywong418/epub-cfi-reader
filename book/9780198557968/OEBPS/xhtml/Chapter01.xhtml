<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html> 
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en" lang="en">
<head>
<meta charset="utf-8"/>
<title>Fundamental ideas</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet.css"/>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div>

<section epub:type="chapter" id="ch01">
<a id="page_1" class="page" style="width:70%">Page 1, 1 Fundamental ideas</a>

<h1 class="main1"><b>1</b><span class="space">&#160;</span><b>Fundamental ideas</b></h1>
<p class="noindent">&#160;</p>

<p class="noindent">Elegant and sophisticated experiments in chemistry are enabling us to understand in increasingly intimate ways the nature of the interactions that lead to chemical change, and to identify chemical species of great complexity present at minute concentrations. Almost all these experiments have depended on the development of sensitive and precise instrumentation that allows measurements to be made and systems to be probed reliably and rapidly. Much of the equipment can be purchased virtually ready for use, although sometimes different elements have to be connected together to satisfy a particular purpose. A &#x2018;black-box&#x2019; approach to chemical instrumentation can be perfectly effective so long as nothing more is asked than the performance that the manufacturers intended. When novel experiments are designed, however, or if apparatus is to be used at the very limits of its potential as is so often the case in creative research, then a different attitude may be needed. It may be necessary, for example, to use the instrumentation in a new way, to make minor modifications to it, or to assemble and connect together units and components to satisfy the special requirements of the work.</p>

<p class="indent3-ka">This book has been written with the intention of introducing chemists to some of the the building blocks and devices that make up the most important instruments used in industry and research. It should provide a basic understanding of how the instrumentation works and thus help towards using what is available both correctly and effectively; the knowledge may perhaps even provide the first step along the path towards devising novel ways of using existing instrumentation and ultimately of developing entirely new tools by which chemistry may be studied.</p>

<p class="indent3-ka">Virtually every scientific instrument encodes the information of interest in some form of electrical signal, which is then manipulated as required, and subsequently decoded into a useful form. Many of the advances of instrumentation have therefore been made possible by improvements in electronics; the advent of solid-state devices, first the transistor and then integrated circuits of ever-increasing complexity, has had a major impact on instruments and techniques in all branches of science and technology. Because of the almost universal application of electrically encoded signals, instrumentation and electronics are closely intertwined. However, it is not the intention of this book to dwell unduly on the intricacies of electronics, although some principles must be understood before the operation of the instruments can be explained. The rest of this chapter is aimed, therefore, at those who have forgotten those principles, or were, perhaps, never taught them. Other readers can skip much of it, but might still find the reminders that it contains of value before they tackle the rest of the book.</p>



<section epub:type="chapter" id="sec_1.1">
<a id="page_2" class="page" style="width:70%">Page 2, 1 Fundamental ideas</a>
<h2 class="h2"><b>1.1</b><span class="space">&#160;</span><b>Current, potential difference, and power</b></h2>


<p class="noindent">This section deals with the most fundamental of electrical quantities. A flow of electrons in a conductor constitutes a <i>current.</i> If the charge that passes each second is one coulomb, the current is one amp (or ampere). For electrons to flow, one end of the conductor has to be more positive than the other. That is, one end is at a higher potential than the other, and there is a <i>potential difference</i> between the ends. It is this potential difference (p.d.) that causes the current to flow. The unit of p.d. is the volt, and p.d. is sometimes referred to in a shorthand way as &#x2018;voltage&#x2019;. We shall adopt this usage throughout the book when it is convenient. Another closely related quantity is the <i>electromotive force</i> (e.m.f.), also measured in volts, and which differs from p.d. only in a subtle way that need not concern us here (<a href="Chapter02.xhtml#page_23">see p. 23</a>).</p>


<p class="indent3-ka">If a current flows in a circuit, then charge moves down the potential gradient, and work can be done equal to the charge multiplied by the potential. Since the current is the charge that flows in unit time, the product of current and voltage is the work done in unit time, which is the <i>power</i> delivered to the system</p>

<figure class="fig1" id="eq_1.1">
<p class="noindent" style="text-indent:13em"><i>P &#x003D; IV</i> <span style="float:right">(1.1)</span></p></figure>

<p class="noindent">where <i>P</i> is the power, <i>I</i> the current and <i>V</i> the voltage. If <i>I</i> is measured in amps and <i>V</i> in volts, then <i>P</i> is in watts.</p>

<aside class="abc" style="margin-top:-11em;" epub:type="sidebar">
<p class="aside">work &#x003D; charge x potential</p>
<p class="aside">power &#x003D; work x time</p>
<p class="aside" style="margin-top:-1em;margin-left:2.8em;">&#x003D; charge x potential/time</p>
<p class="aside" style="margin-top:-1em;margin-left:2.8em;">&#x003D; current x potential</p>
</aside>
</section>

<section epub:type="chapter" id="sec_1.2">
<h2 class="h2"><b>1.2</b><span class="space">&#160;</span><b>Resistance</b></h2>

<p class="noindent">Any real conductor normally offers a <i>resistance</i> to the flow of charge. If it did not, an infinite current would flow for the smallest potential difference. The relationship between the current flowing in the conductor, and the voltage applied across it is embodied in <i>Ohm&#x2019;s law,</i> which states that</p>

<figure class="fig1" id="eq_1.2">
<p class="noindent" style="text-indent:13em"><i>I &#x003D; VI</i><i>R</i><span style="float:right">(1.2)</span></p></figure>

<p class="noindent">where <i>I</i> is the current, <i>V</i> the voltage and <i>R</i> the resistance. If <i>I</i> is measured in amps and <i>V</i> in volts, then <i>R</i> is in ohms.</p>

<aside class="abc" style="margin-top:-5em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.1">
<img src="../images/page_2_1.jpg" alt="images" style="margin-left:3em"/>
<p class="aside"><b>Fig. 1.1</b> The specific resistivity is the resistance between opposite faces of a cube of unit length sides</p></figure></aside>

<h3 class="h3">Resistivity and resistors</h3>
<p class="noindent">The resistance of a conductor increases in proportion to its length, and inversely with the cross-sectional area. For a given length and area, different materials exhibit different resistances. The <i>specific resistivity</i> is the resistance of a sample of unit length and unit cross-sectional area (cf.<a href="#fig_1.1"> Fig. 1.1</a>). Copper and silver are good conductors and have lower specific resistances than many other materials, although aluminium is a relatively good conductor that offers advantages in terms of weight. A <i>resistor</i> is a key electronic component that is designed to offer a chosen resistance. Resistors can be made of lengths of wire (usually wound on a former), thin films of metal coated on glass or <a id="page_3" class="page">Page 3, 1 Fundamental ideas</a>another insulator, or lengths of rod made of carbon (which is a modest conductor). <i>Semiconductors</i> are neither good conductors nor perfect insulators. Some semiconductors, however, possess extraordinary electrical properties that allow the construction of devices such as diodes and transistors (see <a href="#sec_1.5">Section 1.5</a>). Examples include elements such as silicon and germanium, usually &#x2018;doped&#x2019; with an impurity element of a neighbouring group of the periodic table in order to confer the desired properties.</p>

<p class="indent3-ka">Ohm&#x2019;s law may be used very simply to calculate the power dissipated in a circuit in terms of current and resistance or voltage and resistance.  <a href="#eq_1.1">Equations (1.1)</a> and <a href="#eq_1.2">(1.2)</a> need to be combined to give the desired relation, as shown in the margin.</p>

<aside class="abc" style="margin-top:-7em;" epub:type="sidebar">

<p class="aside"><i>P&#x003D;VI; V&#x003D;IR; I&#x003D; V/R</i></p>
<p class="aside" style="margin-top:-0.9em;margin-left:0.6em;">&#x003D; <i>I<sup>2</sup>r</i></p>
<p class="aside" style="margin-top:-0.9em;margin-left:0.6em;">&#x003D; <i>V<sup>2</sup>/R</i></p></aside>

<h3 class="h3">Temperature dependence of resistance</h3>
<p class="noindent">Good metallic conductors usually show a resistance that decreases with
decreasing temperature. Indeed, as absolute zero of temperature is
approached, the resistance of a conductor may also tend towards zero, and the
sample is said to be <i>superconducting.</i> Superconducting magnets, cooled by
liquid helium to around 4K, find several applications in chemical
instrumentation, most notably in nuclear magnetic resonance (n.m.r.)
spectrometers. One major advantage of the superconducting magnet is that
very large currents (and thus high magnetic fields) can be sustained without
appreciable heating. A new class of material that exhibits superconductivity
at much higher temperatures has recently been discovered, and offers the
prospects of improved performance in many electrical applications.</p>

<p class="indent3-ka">Semiconductors often show a <i>negative temperature</i> coefficient of
resistance: that is, their resistance increases with decreasing temperature.</p>

<h3 class="h3">Kirchhoff&#x2019;s laws</h3>
<p class="noindent"><i>Kirchhoff&#x2019;s laws</i> greatly simplify the analysis of electrical circuits, and we
shall have occasion to use the laws later. There are two laws, one applicable
to current and the second applicable to voltage:</p>

<aside class="abc" style="margin-top:-13em;" epub:type="sidebar">
<figure class="fig1">
<img src="../images/page_3_1.jpg" alt="images"/>
</figure></aside>

<ul>
<li>the algebraic sum of the currents flowing into any junction is zero (see <a href="#fig_1.2">Fig. 1.2(a)</a>); and</li>
<li>the algebraic sum of the potential differences around any complete loop in a circuit is zero (see <a href="#fig_1.2">Fig. 1.2(b)</a>).</li>
</ul>



<aside class="abc" style="margin-top:-7em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.2">
<img src="../images/page_3_2.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.2</b> Kirchoff's<br/>
laws: (a) current<br/>
law: (b) voltage law</p>
</figure>
</aside>

<p class="noindent">The meaning of the words &#x2018;algebraic sum&#x2019; means that account has to be taken of the signs of the quantities, so that current flowing out of a junction is counted as negative, and into the junction as positive. The current law is a consequence of the conservation of charge, which cannot be stored in a conductor, and the voltage law is essentially a statement of the conservation of energy, since the work done in moving a charge round a closed loop, where the starting and finishing potentials are the same, must be zero. It is most convenient to illustrate the use of these laws by tackling a real problem that <a id="page_4" class="page">Page 4, 1 Fundamental ideas</a>is of practical importance. Resistors may be connected in <i>parallel</i> or in <i>series,</i> as illustrated in <a href="#fig_1.3">Fig. 1.3(a)</a> and (b) for a system of three resistors <i>R</i><sub>1</sub>, <i>R</i><sub>2</sub>, and <i>R</i><sub>3</sub>. The problem is then to calculate the effective combined resistance, <i>R</i><sub>p</sub> or <i>R</i><sub>s</sub> of the connected components. The application of the current law to the series situation, and the voltage law to the parallel one, shows rapidly that the results are</p>

<figure class="fig1" id="eq_1.4a">
<p class="noindent" style="text-align:center">R<sub>p</sub><sup>-1</sup> = R<sub>1</sub><sup>-1</sup> + R<sub>2</sub><sup>-1</sup> + R<sub>3</sub><sup>-1</sup>. . . .<span style="float:right">(1.4a)</span></p></figure>

<figure class="fig1" id="eq_1.4b">
<p class="noindent" style="text-align:center">R<sub>s</sub> = R<sub>1</sub> + R<sub>2</sub> + R<sub>3</sub>. . . .<span style="float:right">(1.4b)</span></p></figure>


<aside class="abc" style="margin-top:-11em;" epub:type="sidebar">
<figure class="fig1">
<img src="../images/page_4_1.jpg" alt="images"/>
</figure></aside>

</section>

<section epub:type="chapter" id="sec_1.3">
<h2 class="h2"><b>1.3</b><span class="space">&#160;</span><b>Direct and alternating currents</b></h2>
<p class="noindent">An important distinction is drawn between voltages and currents that remain of the same sign at all times and those whose polarity changes and alternates with time. It is usual to refer to the two types as <i>direct current (d.c.)</i> and <i>alternating current (a.c.),</i> regardless of whether it is voltages or currents that are involved. The significance of the division into d.c. and a.c. is that many circuits respond in different ways to them. For the time being, however, we shall examine some of the essential features of a.c.</p>

<aside class="abc" style="margin-top:-7em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.3">
<img src="../images/page_4_2.jpg" alt="images"/>
<p class="aside"><b>Fig. 1.3</b> Resistors in (a) parallel and (b) series</p></figure>



<figure class="fig1" id="fig_1.4">
<img src="../images/page_4_3.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.4.</b> Variation of amplitude with time for a sine wave The angles marked emphasize the cyclic behaviour, and show that the sequence repeats itself after 360&#x00B0;</p></figure></aside>


<h3 class="h3">Sine waves</h3>

<p class="noindent"><a href="#fig_1.4">Figure 1.4</a> is a representation of an a.c. waveform that plays a peculiarly central role in all physics. It is the <i>sinusoidal</i> or <i>sine wave.</i> The figure may be thought of as graph of voltage across two terminals, or of current flowing in a circuit, plotted as a function of time. The ordinate (y-axis) is the time-dependent <i>amplitude</i> of the quantity being measured. The waveform is evidently recurrent or <i>cyclic.</i> Because of this cyclic behaviour, it is often convenient to regard the abscissa (x-axis) as showing how far round the cycle the waveform has progressed. <a href="#fig_1.4">Figure 1.4</a> illustrates the idea by showing that the waveform has repeated itself after 360 degrees, which is the angle of rotation in going round a full circle or cycle. This particular waveform is a sine wave because the amplitude is proportional to the sine of the angle. That is, <i>A</i> &#x003D; <i>A<sub>max</sub></i> sin &#x03B8;, where <i>A</i> is the amplitude at any time, <i>A<sub>max</sub></i> is the maximum amplitude at the crest of the wave, and &#x03B8; is the angle progressed round the cycle. Remember that sin (180 &#x002B;  &#x03B8;) &#x003D; -sin&#x03B8;, and you will see why the second half of the cycle is of opposite polarity to the first, and remember further that sin (360 &#x002B;  &#x03B8;) &#x003D; sin&#x03B8;, and you will see that the behaviour is entirely consistent with the idea of repetition of the waveform after a cycle of 360 degrees. Although these angles have been expressed here in degrees, it is more convenient mathematically to employ units of radians. For those not familiar with these units, a brief explanation is given in <a href="#fig_1.5">Fig. 1.5.</a> The feature to remember is that 360 degrees converts to 2&#x03C0; radians. One further property is of prime significance in dealing with a.c. phenomena, and that is the frequency, <i>f.</i> This frequency is simply the number of cycles that fit into each second, and is measured in <i>hertz,</i> abbreviated Hz. Put another way, the <i>period</i> <a id="page_5" class="page">Page 5, 1 Fundamental ideas</a>between successive cycles is 1 <i>If</i> and the total angle rotated in a time <i>t</i> is <i>2&#x03C0;ft</i> radians (or 360<i>ft</i> degrees). The quantity <i>2&#x03C0;f</i> appears time and again in a.c. formulas, which is one reason for introducing angular measurements in radians in the last paragraph.</p>


<h3 class="h3">The superposition of sine waves</h3>

<p class="noindent">We must now examine why the sine wave is so important in discussions of alternating waveforms. It is simply that all other waveforms can be produced by superposing an appropriate series of sine waves. This property was discovered by Jean Baptiste Fourier, and <i>Fourier synthesis, Fourier analysis,</i> and <i>Fourier transforms</i> are all physical or mathematical techniques or tools that exploit Fourier&#x2019;s discovery. Fourier synthesis is the building up of a more complex waveform through the superposition of sine waves. The total voltage or current is the algebraic sum of the individual sine-wave components, an idea probably familiar in the context of constructive and destructive <i>interference.</i> Fourier analysis is the reverse process of finding out what sine waves go to make up the more complex wave, while Fourier transforms are a mathematical tool used in performing this analysis.</p>


<aside class="abc" style="margin-top:-23em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.5">
<img src="../images/page_5_1.jpg" alt="images" style="margin-left:3em"/>
<p class="aside"><b>Fig. 1.5</b> Definition of an angle &#x03B8; in radians: &#x03B8;<sub>rat</sub> &#x003D; <i>l/r</i> For 360&#x00B0;, <i>I</i> &#x003D; <i>2&#x03C0;r</i> and &#x03C0; &#x003D; <i>2&#x03C0;</i> For 90&#x00B0;, <i>I</i> &#x003D; <i>&#x03C0;r/2</i> and &#x03B8; &#x003D; <i>&#x03B8;/2</i></p></figure></aside>

<h3 class="h3">Periodic waveforms</h3>
<p class="noindent">Let us start by considering waveforms that are repeated continuously in the same way that we have presumed the sine wave to be. Such repeated waveforms are called <i>periodic.</i> Two simple examples are the square wave (<a href="#fig_1.6">Fig. 1.6(a)</a>) and the triangular wave (<a href="#fig_1.7">Fig. 1.7(a)</a>). It turns out that the only sine waves needed to generate <i>any</i> such repeated waveform are those which are harmonics or multiples of some fundamental frequency <i>f</i><sub>0</sub> that is, only frequencies <i>f</i><sub>0</sub>, 2<i>f</i><sub>0</sub>, 3<i>f</i><sub>0</sub>, 4<i>f</i><sub>0</sub>, and so on, are needed. Indeed, for the square and triangular waves, the only frequencies required are the <i>odd</i> harmonics, <i>f</i><sub>0</sub>, 3<i>f</i><sub>0</sub>, 5<i>f</i><sub>0</sub>, .... <a href="#fig_1.6">Figure 1.6(b)</a> shows how the addition of just the first two terms of this series makes the resultant waveform begin to look more like a square wave. Successive addition of the right amounts of the higher odd harmonics makes the resultant waveform approximate more and more closely to the square wave. An important phrase in the last sentence is &#x2018;of the right amounts&#x2019;. The amplitudes have to be selected carefully to get the right result.
For the square wave, the required amplitudes are in the ratio 1:1/3:1/5 <a href="#fig_1.6">Figure 1.6(c)</a> illustrates the amplitudes needed for each harmonic in graphical form. The part played by the amplitude in determining the waveform produced by superposed waves is apparent when the triangular and square waves are compared. Although again only the odd harmonics are needed to synthesize the triangular wave, the amplitudes must this time be in the ratio 1:1/9:1/25. . . ., as indicated in <a href="#fig_1.7">Fig. 1.7(b)</a>.</p>

<aside class="abc" style="margin-top:-33em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.6">
<img src="../images/page_5_2.jpg" alt="images"/>
<p class="aside"><b>Fig. 1.6</b> Fourier synthesis of a square wave: (a) the square wave; (b) the first two components and their sum; (c) the amplitude spectrum</p></figure></aside>

<p class="indent3-ka">The two figures representing the intensities required at different frequencies, <a href="#fig_1.6">Figs 1.6(c)</a> and <a href="#fig_1.7">1.7(b)</a>, are the <i>spectra</i> of the two waveforms. The analogy with optical spectroscopy is complete. Light may consist of a <a id="page_6" class="page">Page 6, 1 Fundamental ideas</a>mixture of monochromatic wavelengths (single frequencies). Some device such as a prism or grating may be used to analyse these wavelengths or frequencies, in order to discover the intensity of each. The result is a spectrum. The opposite procedure, that of mixing light of different colours corresponds to the synthesis of a more complex waveform. In the nomenclature of Fourier transforms, which we shall discuss later, <a href="#fig_1.6">Figs 1.6(a)</a> and <a href="#fig_1.6">1.6(c)</a>, or <a href="#fig_1.7">Figs 1.7(a)</a> and <a href="#fig_1.7">1.7(b)</a>, are representations of conjugate <i>domains,</i> in this case the time domain and the frequency domain: that is, the waveform and its spectrum. The time-domain and frequency-domain figures contain the same information expressed in different ways. It is worth noting that a simple continuous sine wave corresponds to just one frequency and thus to a single line in the frequency domain.</p>

<aside class="abc" style="margin-top:-13em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.7">
<img src="../images/page_6_1.jpg" alt="images" style="margin-left:0.5em"/>
<p class="aside"><b>Fig. 1.7</b> Fourier synthesis of a triangular wave: (a) the waveform; (b) the amplitude spectrum</p></figure></aside>


<p class="indent3-ka">Our two examples were chosen for their simplicity. In other cases, more complicated mixtures of amplitudes for different frequencies are required, but the principle remains the same. Sometimes the waveforms for the different frequencies must be displaced in origin from each other. Such a displacement is called a <i>phase shift,</i> and corresponds to a certain angle, &#x0444;, of offset for a given waveform (see <a href="#fig_1.8">Fig. 1.8</a>). In terms of our equation for an individual sine wave, the equation <i>A</i> &#x003D; <i>A<sub>max</sub>sin</i> &#x03B8; becomes modified to A &#x003D; A<sub>max</sub>sin (&#x03B8; + &#x0444;), and, in the synthesis of the complex waveform, each component may have its own individual value of &#x0444;.</p>


<aside class="abc" style="margin-top:-3em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.8">
<img src="../images/page_6_2.jpg" alt="images" style="margin-left:0.5em"/>
<p class="aside"><b>Fig. 1.8</b> Meaning of the phase angle &#x0444; for a single frequency sine wave</p>
</figure></aside>


<h3 class="h3">Non-periodic waveforms</h3>

<p class="noindent">The statements made in the preceding paragraphs that all waveforms can be synthesized by the superposition of <i>harmonically related</i> sine waves, with correctly chosen amplitudes and phases has always been qualified by the requirement that the more complex waveform be repeated infinitely in time. The question must then arise about the feasibility of similarly representing a truncated waveform that is not repeated for ever, or even a pulse. The answer is that <i>any</i> waveform can be synthesized by the superposition of sine waves, but that the frequencies of these components will be a simple harmonic series only if the complex waveform is repeated indefinitely. In all other cases, the sine waves required consist of a continuous spread of frequencies rather than the individual &#x2018;monochromatic&#x2019; values encountered so far. In the terms of spectroscopy, the discrete line spectra that correspond to the infinitely repeated waveforms have become replaced by a continuum. The reason for the difference between the sine-wave components of infinitely long wavetrains and those limited in time is not hard to see. A waveform synthesized from a finite number of discrete components is bound ultimately to repeat itself. The more components there are, and the closer they are together, the longer it takes for the repetition to occur. Only if there is an infinite number of frequencies will the pattern be confined to one period of time when all the sine waves interfere constructively, or are &#x2018;in phase&#x2019;. At earlier and at later times, there is inevitably destructive interference between the infinite number of sine waves. <a href="#fig_1.10">Fig. 1.10</a> attempts to illustrate the way in which the superposition of a wide <a id="page_7" class="page">Page 7, 1 Fundamental ideas</a>range of frequencies produces a waveform that dies away on either side of a transient region in the centre; this region becomes narrower as the range of frequencies employed to synthesize it becomes wider. What cannot be shown</p>


<figure class="fig1" style="margin-left:5em;" id="fig_1.10">
<img src="../images/page_7_2.jpg" alt="images"/>
<p class="noindent">&#160;</p>
<figcaption style="text-align:left;margin-left:-5.4em;"><b>Fig. 1.10</b> A pulse has Fourier components of many frequencies needed for destructive interference at all times before and after the pulse. A narrower pulse (b) demands a wider range of frequencies than the broader pulse (a)</figcaption></figure>

<aside class="abc" style="margin-top:-35em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.9">
<img src="../images/page_7_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.9</b> Non-periodic waveforms: (a) a pulse; (b) a burst of oscillations</p></figure></aside>


<p class="noindent">in such a diagram is that the frequencies employed must make up a continuum in both cases. It is, however, very easy to show the spectrum in the frequency domain corresponding to the two waveforms in the time domain, and the two pairs of representations are given in <a href="#fig_1.11">Fig. 1.11</a>. A voltage pulse corresponds to yet another kind of spectrum in the frequency domain. As the pulse gets shorter, so the spread of frequencies gets larger, until in the limit of an infinitely narrow spike, all frequencies are represented at the same amplitude. The spectrum is then said to be &#x2018;white&#x2019; by analogy with the visible spectrum of white light that contains all colours mixed together. At the other extreme, an unvarying d.c. voltage corresponds to an infinitely narrow spectrum containing only zero frequency. To conclude the sequence of time and frequency domain pictures, it is worthwhile including the pure continuous sine wave with which we started out.</p>


<p class="noindent"><a id="page_8" class="page">Page 8, 1 Fundamental ideas</a></p>
<h3 class="h3">The consequences of non-sine wave voltages</h3>

<p class="noindent">Many of the ideas presented in the previous sections have applications and consequences that will become apparent as the theme of this book develops. One aspect is in understanding how Fourier transform spectroscopy works. However, there are some other comments that it is convenient to make here. First, it is possible to interpret the response of many electrical and electronic circuit elements to complex applied voltage waveforms by considering the response to each individual sine wave component and summing the result. Some circuit elements are <i>non-linear</i> in their responses, in which case they produce an output which is not a pure sine wave and which thus contains harmonics. Secondly, it is evident that the electronic devices used in any instrumentation must be capable of responding to the highest frequencies present in a complex waveform if the output is not to be distorted from the input. A typical example that can readily be understood now is the frequency response demanded of amplifiers intended to increase the voltage of a pulse. Because, as we have just seen, a short pulse is equivalent to a very wide range of frequencies, the <i>bandwidth</i> of the amplifier must be sufficient to allow the pulse to be transmitted without information being lost. We shall return to this question of information and bandwidth shortly.</p>

<aside class="abc" style="margin-top:-20em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.11">
<img src="../images/page_8_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.11</b> Some waveforms (time domain) and their Fourier transforms (frequency domain)</p></figure></aside>

<p class="indent3-ka">Since this is a book for chemists, it would be remiss of us not to look at another consequence of the frequency spread of short wavetrains. We know now that the shorter the train, the larger the spread. It is this behaviour that leads to the width of spectral lines being larger for transitions in which the excited state has shorter lifetimes. The exact relation between lifetime and linewidth can be determined from the quantitative forms of the information in time and frequency domains (see next section). Only a small step leads us to the <i>Heisenberg uncertainty principle,</i> which in its usual form links uncertainty in position and momentum of a (quantum) body or particle. Because particles also behave as waves, a localized particle corresponds to a wave that must be truncated because its position in space is known. The frequency is therefore spread out (uncertain), and since frequency and energy are related, the energy and ultimately the momentum are also uncertain.</p>

<h3 class="h3">Fourier transforms: a first glance</h3>

<p class="noindent">We have come, almost unwittingly, to a point at which the technique of Fourier transformation should be intelligible. Fourier transform methods play an increasingly important part in much of chemical instrumentation. They are used to improve enormously the sensitivity or resolution of several spectroscopic and related techniques. They are also used to tailor the behaviour of electronic devices, and they are used in the refinement and interpretation of the data obtained in the experiments. We shall return to a more detailed discussion of various kinds of Fourier spectroscopy, but it is appropriate here to look at the Fourier transform in the light of the material just presented. In outline, the Fourier transform is a mathematical technique that permits its user to convert information presented in the time domain so <a id="page_9" class="page">Page 9, 1 Fundamental ideas</a>that it appears in the frequency domain, or <i>vice versa.</i> Other important pairs of domains are also connected by Fourier transformation, and they include distance and reciprocal length. The mathematical expressions involved are not unduly complicated, but do not really concern us yet. However, they can be applied analytically only to rather simple situations. The advent of high-speed computers has dramatically widened the scope of the methods, because numerical calculations of Fourier transforms can now be performed so rapidly as to be essentially in &#x2018;real time&#x2019;, and the operation is entirely routine. The really important point about the Fourier transform is that it consists of a summation (or integration) of sine functions corresponding in mathematics exactly to the addition of sine wave amplitudes, voltages, or currents that we have been considering in physics. As a result, the complex waveform in the time domain is identical to the Fourier transform of the frequency spectrum. One remarkable property of the Fourier transform is that, apart from some amplitude scaling factors and sign changes, if a function is transformed, and that transform is subjected to the transformation procedure a second time, then the original function is regenerated. Given that the waveform in the time domain is the first Fourier transform of the spectrum of the frequency domain, it follows that the second Fourier transform (properly speaking the <i>inverse</i> transform) of the waveform is the frequency spectrum itself. What all this means is that it is possible to shift simply and painlessly from one domain to the other.</p>

<aside class="abc" style="margin-top:-23em;" epub:type="sidebar">

<p class="aside">The Fourier transform.</p>

<p class="aside">If a spectrum in the frequency domain is written g(<i>f</i>), then a simplified Fourier transform G(<i>t</i>) in the time domain can be written</p>


<p class="fig1" style="margin-left:3em"> 

<math xmlns="http://www.w3.org/1998/Math/MathML">

 <mrow>
  <mi>G</mi><mo stretchy='false'>(</mo><mi>t</mi><mo stretchy='false'>)</mo><mo>=</mo><mstyle displaystyle='true'>
   <mrow><munderover>
    <mo>&#x222B;</mo>
    <mrow>
     <mo>&#x2212;</mo><mi>&#x221E;</mi>
    </mrow>
    <mi>&#x221E;</mi>
   </munderover>
   <mrow>
    <mi>g</mi><mo stretchy='false'>(</mo><mi>f</mi><mo stretchy='false'>)</mo><mi>cos</mi><mn>2</mn><mi>&#x03C0;</mi><mi>f</mi><mi>t</mi><mi>d</mi><mi>f</mi>
   </mrow>
  </mrow>
  
 </mstyle>
</mrow>
</math>

</p>

<p class="aside">and the inverse transform recovers g<i>(f)</i></p>

<p class="fig1" style="margin-left:3em"> 

<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow>
  <mi>G</mi><mo stretchy='false'>(</mo><mi>f</mi><mo stretchy='false'>)</mo><mo>=</mo><mstyle displaystyle='true'>
   <mrow><munderover>
    <mo>&#x222B;</mo>
    <mrow>
     <mo>&#x2212;</mo><mi>&#x221E;</mi>
    </mrow>
    <mi>&#x221E;</mi>
   </munderover>
   <mrow>
    <mi>G</mi><mo stretchy='false'>(</mo><mi>t</mi><mo stretchy='false'>)</mo><mi>cos</mi><mn>2</mn><mi>&#x03C0;</mi><mi>f</mi><mi>t</mi><mi>d</mi><mi>t</mi>
   </mrow>
  </mrow>
  
 </mstyle>
</mrow>
</math>
</p>


<p class="aside">(note that these simplified forms apply only to functions g(<i>f</i>) and G(<i>t</i>) that are symmetrical about <i>f</i> &#x003D; 0 or <i>t &#x003D;</i> 0; sin as well as cos terms arise in other cases).</p>
</aside>

<p class="indent3-ka">As we have insisted repeatedly, the information contained in the time and frequency domains is identical, and that is made clearer now by seeing how the two are mathematically linked and how they can be interconverted. It is curious that most of us seem to think more readily of waveforms in the time domain than of the spectra in the frequency domain. Instruments exist that allow either domain to be examined. The <i>oscilloscope</i>, to be described in the next chapter, shows waveforms as a function of time, while the <i>spectrum analyser</i> allows the amplitudes of the frequency components to be displayed. The oscilloscope is, however, the more common device, and was certainly developed much earlier than the spectrum analyser; it is possibly these factors that colour our way of thinking.</p>

<h3 class="h3">Information and bandwidth</h3>

<p class="noindent">The reason for using any piece of instrumentation is to obtain information about an experiment. There is a precise quantitative definition of information (which is very closely related to the statistical definition of entropy), but the qualitative idea will suffice here. Information is conveyed in all sorts of different ways; examples include the speech or music in a radio programme, the picture on a television set, the data stream within a computer, the pattern on an oscilloscope, the trace on a chart recorder, and the movement of the needle on the scale of a meter. An underlying principle that governs the transfer of information is that it is only <i>changes</i> in the observed phenomenon that allow the information to be extracted. A truly continuous and unvarying response, such as a constant d.c. voltage or a sine or other periodic a.c. <a id="page_10" class="page">Page 10, 1 Fundamental ideas</a>waveform, cannot convey information. The reading on a meter provides information only if it is known that the reading is something different when the voltage is not present, and a tone heard on the radio conveys information only if a tone of different loudness (amplitude) or pitch (frequency) is known to be possible. In both these examples, and in every case besides, the changes required prevent the signal from being a pure d.c. or strictly periodic a.c. one. The more information that must be conveyed in a given time, the more rapidly must the amplitude or frequency be varied. On a chart recorder, the levels might change over a period of seconds or more, information conveyed by a voice corresponds to a few thousand variations in each second, while music has roughly ten times more information. A high-definition television picture needs several million changes of amplitude a second to be built up.</p>

<p class="indent3-ka">The treatment of non-periodic waveforms presented in previous sections should have made it clear that the alterations in the waveform needed to convey information will correspond in the frequency domain to a continuum of frequencies, the spread of which will increase as the rate of transfer of information increases. This spread is of the order of the rate of alteration of the waveform, and in our examples is tens of kHz (kilohertz &#x003D; 10<sup>3</sup> Hz) for audible sounds and several MHz (megahertz &#x003D; 10<sup>6</sup>Hz) for television pictures. There are consequences of the widened spectrum that have to be allowed for in scientific instrumentation and in all other systems that have to carry information. Electronic and other devices must be able to respond to the highest frequencies present, and they must be able to process the range of frequencies equally and without favouring some parts of the spectrum more than others. If such processing is not possible, then the signal will be distorted and information lost. The devices must therefore pass an adequate range of frequencies, in which case they are said to possess an adequate bandwidth. We met this term earlier in explaining the bandwidth demands of amplifiers for pulses, but the bandwidth requirement does not just apply to amplifiers and other electronic devices, but rather to the complete system. A meter or pen recorder can only register changes slower than the maximum rate at which the needle or pen can move, and a music system cannot convey to the listener frequencies beyond the capabilities of the loudspeaker.</p>


<p class="indent3-ka">It might be thought that suitable bandwidths could be attained, at least in principle, to handle any desired information rate, and that practical limitations might ultimately succumb to improvements in the devices. There is, however, a more basic limitation to the rate at which information can be handled. Noise is <i>always</i> present in any electrical system, because of the thermal motions of electrons and their statistical behaviour even if there are no external sources of noise. The reduction of noise is part of the challenge in inventing and improving chemical instrumentation, and <a href="Chapter05.xhtml">Chapter 5</a> deals with the subject in detail. The signal has to compete with this noise, and the precision of measurements is degraded as a result. The extent of the degradation is determined by the relative contributions of the signal and the noise; this relative magnitude obtains quantitative expression in the <i>signal-to-noise ratio (S/N).</i> Unavoidable noise in electrical systems is &#x2018;white&#x2019;; that is, its <a id="page_11" class="page">Page 11, 1 Fundamental ideas</a> amplitude is the same at all frequencies. The signal of interest is usually confined to a relatively narrow band about a fixed frequency, the bandwidth being determined by the information transfer rate as just discussed. An increase in the S/N ratio can thus be achieved by making the bandwidth of the signal processing devices as small as possible consistent with the desired information rate. Further increases in the S/N ratio can only be achieved if the information rate is reduced. There is thus a trade-off between the rate at which information can be transferred and the precision with which it can be obtained. Perfect precision is possible only if no information at all is transferred&#x0021; The correlation between the uncertainty of the measurements (that is, the inverse of the S/N ratio) and the number of measurements that can be made looks very much like an uncertainty principle, and it arises for the same reasons as the quantum mechanical one. A fundamental limit is placed on the information transfer rate and the S/N ratio. Whether or not that limit affects the viability of an experiment depends, of course, on the signal level generated in the experiment and on the minimum precision acceptable in it as well as on the rate at which it is desired that the observations be made.</p>



<aside class="abc" style="margin-top:-21em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.12">
<img src="../images/page_11_1.jpg" alt="images" style="margin-left:2em"/>
<p class="aside"><b>Fig. 1.12</b> Some complex waveforms can be regarded as the sum of a constant d.c. term and a time-dependent term</p></figure></aside>


<h3 class="h3">Combined d.c. and a.c.</h3>

<p class="noindent">Although we have not discussed the question so far, many waveforms encountered in scientific experiments have a constant term that has to be added to the time-dependent sine-wave terms in the Fourier synthesis. For our purposes, it is most convenient to think of such signals as being a combination of a mean d.c. level and an a.c. component (<a href="#fig_1.12">Fig. 1.12</a>). In that case, the d.c. component is really constant, and all the information is conveyed by the a.c. signal. Even a slowly varying &#x2018;d.c.&#x2019; signal, or one that is switched on or off, can be viewed in this way. If the information is carried by the a.c. alone, then it may be desirable to separate the a.c. and d.c. parts, and that can be achieved with the capacitors, inductors, and transformers that are introduced in the next section.</p>


<aside class="abc" style="margin-top:-14em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.13">
<img src="../images/page_11_2.jpg" alt="images" style="margin-left:5em"/>
<p class="aside"><b>Fig. 1.13</b> The symbol for a capacitor. Capacitance, C, is defined by the equation</p></figure>

<p class="aside">C = q/V</p></aside>

</section>


<section epub:type="chapter" id="sec_1.4">
<h2 class="h2"><b>1.4</b><span class="space">&#160;</span><b>Capacitors, inductors, and transformers</b></h2>

<p class="noindent">In this section we discuss some very simple circuit elements whose response, in distinction to that of resistors, is dependent on the frequency of the applied voltage or current. Because of this frequency dependence, the devices to be considered behave in completely different ways according to whether the applied signal is d.c. or a.c.</p>

<aside class="abc" style="margin-top:-13em;" epub:type="sidebar">

<p class="aside">where <i>q</i> is the charge stored in the capacitor and <i>V</i> the voltage across it The energy stored in the capacitor is &#x00BD;<i>CV<sup>2</sup></i></p>

<p class="aside">For capacitors <i>C<sub>1</sub>, C<sub>2</sub>, C<sub>3</sub>,</i> etc in series, the total capacitance <i>C<sub>s</sub> </i>is given by the expression </p>

<p class="fig1" style="margin-left:3em"> 

<math xmlns="http://www.w3.org/1998/Math/MathML">

 <mrow>
  <mfrac>
   <mn>1</mn>
   <mrow>
    <msub>
     <mi>C</mi>
     <mi>S</mi>
    </msub>
    
   </mrow>
  </mfrac>
  <mo>=</mo><mfrac>
   <mn>1</mn>
   <mrow>
    <msub>
     <mi>C</mi>
     <mn>1</mn>
    </msub>
    
   </mrow>
  </mfrac>
  <mo>+</mo><mfrac>
   <mn>1</mn>
   <mrow>
    <msub>
     <mi>C</mi>
     <mn>2</mn>
    </msub>
    
   </mrow>
  </mfrac>
  <mo>+</mo><mfrac>
   <mn>1</mn>
   <mrow>
    <msub>
     <mi>C</mi>
     <mn>3</mn>
    </msub>
    
   </mrow>
  </mfrac>
  
 </mrow>
</math>

</p>

<p class="aside">while for the capacitors in parallel the expression is</p>

<p class="aside"><i>C<sub>P</sub> &#x003D;</i> C<sub>1</sub> &#x002B; <i>C<sub>2</sub> &#x002B; C<sub>3</sub></i></p>

</aside>

<h3 class="h3">Capacitors and capacitative reactance</h3>

<p class="noindent">In its simplest form, a capacitor merely consists of two plates separated by an insulating medium. Such a device cannot conduct d.c. currents because of the insulating gap. It can, however, store charge (and hence energy) because application of a potential difference across the plates allows electrons to be drawn from one plate and deposited on the other. The <i>capacitance C</i> is <a id="page_12" class="page">Page 12, 1 Fundamental ideas</a>
defined as the ratio <i>q/V</i> where <i>q</i> is the stored charge and <i>V</i> is the voltage producing it. The basic unit used for C is the farad (F), although it is excessively large to be generally useful, and microfarads (&#x00B5;F), nanofarads (nF), and picofarads (pF) are more frequently encountered. Although some capacitors are constructed with air gaps, it is usual in practical capacitors to increase the amount of charge that can be stored by replacing the air by some other insulating material that has a high <i>relative permittivity</i> (sometimes called <i>dielectric constant).</i> Often, also, provision is made for the surface area to be increased, for example by making the plates of lengths of metal foil, and rolling them up with the separating dielectric material to make a compact capacitor. Capacitors joined together in parallel show a total capacitance equal to the sum of the individual values; in series, the capacitance is decreased.</p>

<aside class="abc" style="margin-top:-15em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.14">
<img src="../images/page_12_1.jpg" alt="images"/>
<p class="aside"><b>Fig. 1.14</b> The charging and discharging of a capacitor through a resistor</p></figure></aside>


<p class="indent3-ka">The charge that may be stored in a capacitor has some direct applications for chemical experiments, because the charge can be built up relatively slowly, but discharged very rapidly. The photographic flash lamp is a familiar example of this use of stored charge, but, at higher energy levels, the discharge of capacitors is used in <i>flash photolysis</i> and in <i>pulsed laser</i> operation.</p>


<aside class="abc" style="margin-top:-2em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.15">
<img src="../images/page_12_2.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.15</b> The current and voltage waveforms for the circuit of Fig. 1.14 if the input voltage is an a.c. sine wave Note that the current &#x2018;leads&#x2019; (is in advance of) the driving voltage by 90&#x00B0; (&#x03C0;/2 radians)</p></figure>

<p class="noindent">&#160;</p>
<p class="aside">The capacitative reactance, <i>X<sub>c</sub>,</i> is given by the expression</p>

<p class="fig1" style="margin-left:3em"> 

<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow>
  <msub>
   <mi>X</mi>
   <mi>c</mi>
  </msub>
  <mo>=</mo><mfrac>
   <mn>1</mn>
   <mrow>
    <mn>2</mn><mi>&#x03C0;</mi><mi>f</mi><mi>C</mi>
   </mrow>
  </mfrac>
  
 </mrow>
</math>
</p>

</aside>




<p class="indent3-ka">While a current cannot pass through a capacitor in the steady state, since charge is transferred when the potential of the plates is changing, there is certainly a transient current present whenever the potential changes. These ideas are illustrated in <a href="#fig_1.14">Fig. 1.14.</a> There is thus a special situation when an alternating voltage is applied, because the capacitor is being continuously charged, discharged, and then charged to the opposite polarity. A current must then be flowing in the circuit at all times, because the steady-state situation is never reached. The capacitor appears to be acting as a conducting resistance in the circuit rather than as an insulator. The resistance is, however, rather peculiar because it is frequency dependent. For a given capacitance, the value is high for low frequencies where the rate of change of charge is small, while at higher frequencies this resistance becomes smaller. There is another important property of the relation between current and voltage that distinguishes the behaviour of the capacitor from that of an ordinary resistor. The current flowing is a maximum when the rate of change of voltage is highest, since that is when the charge also has to be shifted at the maximum rate. The rate of change of voltage is maximum at the crossing points of zero voltage for a sine wave (at 0, 180, 360 . . . degrees, or 0,&#x03C0;, <i>2&#x03C0;</i> radians). At the crests and troughs of the applied voltage waveform, the voltage is momentarily constant, so that the current flowing is zero. In other words, the voltage applied to the circuit and the current flowing in it are shifted in time or phase with respect to each other, as indicated in <a href="#fig_1.15">Fig. 1.15</a>, by one quarter of a cycle (90 degrees or <i>&#x03C0;/2</i> radians). The current waveform precedes the voltage waveform, and the current is therefore said to <i>lead</i> the voltage by 90 degrees. For those who want to think a little more quantitatively about the behaviour, it is necessary only to remember that the rate of change of the voltage is its differential, that the differential of a sine function is a cosine, and that a cosine function leads its sine equivalent by 90 degrees. Because the<a id="page_13" class="page">Page 13, 1 Fundamental ideas</a> behaviour of the capacitor towards a.c. is rather different from that of a true resistor, the special name <i>reactance</i> is given to the effective resistance. For a capacitor, it is usual to specify the quantity as <i>capacitative reactance</i> and to give it the symbol <i>X<sub>c</sub>;</i> like resistance, it is measured in ohms.</p>

<aside class="abc" style="margin-top:-5em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.16">
<img src="../images/page_13_1.jpg" alt="images" style="margin-left:2em"/>
<p class="aside"><b>Fig.</b> 1.16 The symbol for an inductor</p></figure></aside>


<h3 class="h3">Inductors and inductive reactance</h3>

<p class="noindent">A coil of wire constitutes an <i>inductor,</i> a device that is in many ways the counterpart of the capacitor. The coil is said to possess the property of <i>self-inductance,</i> which is given the symbol <i>L,</i> and is measured in henries. The behaviour of inductors is a consequence of the magnetic field that is produced when a current flows in a conductor. If a voltage is applied across a coil of wire, a magnetic field begins to grow. However, since this changing magnetic field is cutting a conductor, it induces a voltage in the conductor (a &#x2018;back e.m.f.&#x2019;) that opposes the voltage applied externally. As a result, the current flowing is initially small and builds up to its limiting value only as a steady-state is reached in the magnetic field. If the voltage is interrupted, the magnetic field collapses, again inducing a voltage in the coil that opposes the change of current flowing in the circuit. The magnitude of the effect will depend on the magnetic field produced and the induced voltage. For a particular geometry of coil, the inductance is proportional to the number of turns of wire. The inductance can be enhanced by inserting a suitable material with a high <i>permeability</i> in or around the coil; common materials used are soft iron and ferrite.</p>


<p class="indent3-ka">As with capacitors, the behaviour of inductors is most clearly demonstrated with a.c. voltages. If the conductor were perfect, at d.c. it would exhibit no resistance, and the current flowing in the circuit would be limited only by the resistance of the source. With a.c., however, the induced voltages reduce the current that flows, and the effective resistance of the circuit is increased over the true &#x2018;ohmic&#x2019; resistance. The inductor is therefore exhibiting <i>inductive reactance,</i> to which the symbol <i>X<sub>L</sub></i> is given. This reactance <i>increases</i> with increasing frequency (compare with the behaviour of capacitative reactance, which falls with increasing frequency). The opposition to the change in applied voltage is maximum when the voltage is just going through zero and is changing magnitude most rapidly, and it is zero at the crests and troughs of the applied voltage waveform. As with capacitors, there is thus a 90 degree phase shift between voltage and current. In the case of inductors, the current does not reach its positive maximum until the applied voltage is passing through zero on its way from the positive to negative half cycle. The current thus <i>lags</i> the voltage by 90 degrees.</p>


<aside class="abc" style="margin-top:-31em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.17">
<img src="../images/page_13_2.jpg" alt="images" style="margin-left:0.5em"/>
<p class="aside"><b>Fig. 1.17</b> Diagrams for the rise and fall of current in an inductor analogous to those shown for a capacitor in <a href="#fig_1.14">Fig. 1.14</a></p></figure>

<p class="aside">In an a.c. circuit, the current &#x2018;lags&#x2019; the voltage by 90&#x00B0;</p>
<p class="aside">The inductive reactance, <i>X<sub>L</sub>,</i> is given by the expression</p>
<p class="aside">X<sub>L</sub> = 2&#x03C0;<i>f</i>L</p>
</aside>

<h3 class="h3">Circuits combining <i>L, C,</i> and <i>R:</i> impedances</h3>

<p class="noindent">We have seen that capacitors and inductors behave towards a.c. as though they possess a resistance that we have termed reactance. The reactance changes with frequency and imposes a 90 degree phase shift on the current with respect to the applied voltage. The question then arises of how to deal with<a id="page_14" class="page">Page 14, 1 Fundamental ideas</a> circuits in which inductors, capacitors, and resistors are connected in series and parallel. If inductive (L) components, or capacitative (C) ones, are present alone, then the total reactance is obtained by using the formulas for resistance, <a href="#eq_1.4a">eqns (1.4a)</a> and <a href="#eq_1.4b">(1.4b)</a>, substituting <i>X<sub>L</sub></i> or <i>X<sub>c</sub></i> for <i>R.</i> The combined circuit still behaves just as a combined <i>L</i> or C, and the phase relationships between current and voltage are those established already. If <i>L</i> and C are both present, then <i>X<sub>L</sub></i> and (-X<sub>c</sub>) can be used together, since the inductive current is 180 degrees out of phase with the capacitative current and subtracts from it. The situation becomes more complicated if resistance is present together with one of the reactive elements because of the phase shifts between the different current components. In fact, it is not difficult to show that the resultant effect of resistance and reactance is equivalent to an opposition to the flow of current which is the vector sum of all the reactive and resistive components. This net opposition is called the <i>impedance,</i> Z, of the circuit. To obtain the magnitude of the impedance it is necessary only to use Pythagoras&#x2019;s theorem (<a href="#fig_1.18">Fig. 1.18</a>). For example, for <i>L, C,</i> and <i>R</i> all in series, the result is</p>

<aside class="abc" style="margin-top:-19em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.18">
<img src="../images/page_14_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.18</b> A vector diagram showing how the complex impedance Zina series circuit containing <i>C, L,</i> and <i>R</i> can be calculated from the capacitative and inductive reactances and the resistance in the circuit</p></figure></aside>

<figure class="fig1" id="eq_1.5">
<p class="noindent" style="text-align:center">Z &#x003D;	{R<sup>2</sup>&#x002B; (X<sub>L</sub>-X<sub>c</sub>)<sup>2</sup>}<sup>&#x00BD;</sup><span style="float:right">(1.5)</span></p></figure>

<p class="noindent">The phase can also be obtained from the vector diagram, but it is the magnitude of Z that is often most important. It is worth noting that the term impedance is often used when there are purely reactive or purely resistive elements present: the word is used to describe the opposition to the flow of current in a general way. Like resistance and reactance, it is, of course, measured in ohms.</p>

<aside class="abc" style="margin-top:-8em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.19">
<img src="../images/page_14_2.jpg" alt="images" style="margin-left:0.5em"/>
<p class="aside"><b>Fig. 1.19</b> High-pass and low-pass filters</p></figure></aside>

<h3 class="h3">Circuits combining <i>L, C,</i> and <i>R:</i> the frequency response</h3>

<p class="noindent">One of the most important uses of capacitors and inductors is in frequency-dependent circuits. At its most simple, this use can be looked on as a means of separating a.c. and d.c. signals. Consider a mixed a.c. and d.c. signal (<a href="#fig_1.19">Fig. 1.19(a)</a>) applied to a resistor, but through a capacitor (<a href="#fig_1.19">Fig. 1.19(b)</a>). The capacitor passes the a.c. component but not the d.c. one (<a href="#fig_1.19">Fig. 1.19(c)</a>), so that the a.c. has been separated from the d.c. Made up the other way round, with the signal applied to a capacitor via a resistor (<a href="#fig_1.19">Fig. 1.19(d)</a>), only the d.c. remains (<a href="#fig_1.19">Fig. 1.19(e)</a>), because the capacitor acts to shunt away the a.c. component. These circuits constitute a <i>high-pass filter</i> and a <i>low-pass filter</i> and are very important building blocks in instrumentation. How well the a.c. component is retained in the high-pass filter or rejected in the low-pass one is a function of the component values and the frequency. The characteristic charging or discharging time in circuit <a href="#fig_1.19">1.19(b)</a>, the so-called time constant, is equal to <i>RC,</i> and the frequency 1/(2&#x03C0;<i>RC)</i> marks a point at which the behaviour of both high- and low-pass filters alters markedly. Frequencies much higher than this point are passed without attenuation through the high-pass filter, but lower frequencies are attenuated by a roughly constant amount for each halving of frequency (<a href="#fig_1.20">Fig. 1.20(a)</a>). The low-pass filter shows converse behaviour (<a href="#fig_1.20">Fig. 1.20(b)</a>).</p>

<p class="indent3-ka"><a id="page_15" class="page">Page 15, 1 Fundamental ideas</a>&#160;&#160;&#160;&#160; Inductors can also be used to make up high- and-low pass filters, as shown in <a href="#fig_1.19">Fig. 1.19(f)</a> and (g). Consideration of the frequency-dependent reactance of inductors will show that the parallel circuit (<a href="#fig_1.19">Fig. 1.19(f)</a>) is this time the high-pass fdter.</p>

<aside class="abc" style="margin-top:-5em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.20">
<img src="../images/page_15_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.20</b> Frequency responses of the high- and low- pass filters: (a) for the circuit of <a href="#fig_1.19">Fig. 1.19(b)</a>; (b) for the circuit of <a href="#fig_1.19">Fig. 1.19(d)</a></p></figure></aside>


<p class="indent3-ka">A combination of inductors and capacitors can likewise be used to generate desired frequency-dependent characteristics in a hybrid circuit. The most interesting behaviour of these circuits comes about because the current lags the voltage by 90 degrees in the inductor, but leads it by 90 degrees in the capacitor, as we noted earlier; <a href="#eq_1.5">eqn (1.5)</a> shows that, without resistance <i>(R &#x003D;</i> 0), the net impedance of an inductor and capacitor in series is <i>(X<sub>L</sub>-X<sub>C</sub>).</i> But what happens if <i>X<sub>L</sub> - X<sub>C</sub>?</i> The reactance must go to zero and the inductor-capacitor pair present a short circuit to the driving voltage. The system is said to constitute a <i>resonant circuit.</i> The frequency at which resonance occurs is obtained by substituting the formulas for <i>X<sub>L</sub></i> and <i>X<sub>c</sub>. A</i> parallel <i>LC</i> circuit is also resonant at the same frequency, but presents <i>infinite</i> impedance at resonance. Series and parallel resonant circuits can thus be used to construct filters that reject the resonant frequency <i>&#x007B;notch filters)</i> or that pass the resonant frequency alone <i>(pass filters),</i> as illustrated in <a href="#fig_1.21">Fig. 1.21</a>. Exactly these kinds of <i>tuned circuit</i> are used in a radio receiver to allow the signal of interest to pass and to reject all others. For purely reactive inductors and capacitors, the resonance would be infinitely sharp and the impedance at resonance truly zero for the series circuit or infinity for the parallel one. Because there are always resistive components in real components, this hypothetical situation does not occur in practice. A quantity <i>Q,</i> the <i>quality factor,</i> is a measure of how closely the practical circuit approaches the theoretical one. A similar g-factor is encountered in many other situations where resonance is involved, ranging from violin construction to laser technology.</p>

<aside class="abc" style="margin-top:-12em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.21">
<img src="../images/page_15_2.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.21</b> Series and parallel <i>L-C</i> resonant circuits. The frequency of resonance, <i>f</i> is given by the expression</p></figure>

<p class="fig1" style="margin-left:0em"> 

<math xmlns="http://www.w3.org/1998/Math/MathML">

 <mrow>
  <msub>
   <mi>f</mi>
   <mi>r</mi>
  </msub>
  <mo>=</mo><mfrac>
   <mn>1</mn>
   <mrow>
    <mn>2</mn><mi>&#x03C0;</mi><msup>
     <mrow>
      <mo stretchy='false'>(</mo><mi>L</mi><mi>C</mi><mo stretchy='false'>)</mo>
     </mrow>
     <mrow>
      <mn>1</mn><mo>/</mo><mn>2</mn>
     </mrow>
    </msup>
    
   </mrow>
  </mfrac>
  
 </mrow>
</math>

</p>

</aside>


<p class="indent3-ka">The properties of the filters just described can be improved in various ways that involve more complicated circuits, perhaps incorporating amplifiers. The rate of change of attenuation with frequency, and the sharpness of the &#x2018;knee&#x2019;, in the high- and low-pass filters can be modified for different purposes. Notch and band-pass tuned circuits can also be constructed from <i>RC</i> components alone. In all cases, however, the principles just set out lie behind the more sophisticated designs.</p>

<h3 class="h3">Transformers</h3>

<p class="noindent">The transformer is another very important and frequently encountered component. In essence, it consists of two coils of wire arranged so that a magnetic field produced by passing a current through one coil is also experienced by the other coil. If an alternating current is passed through the <i>primary</i> coil, the alternating magnetic field will induce a voltage in the <i>secondary</i> coil. With d.c. passing through one winding there is no induced current in the secondary because the magnetic field is constant. The transformer can thus be used to isolate completely an a.c. signal from a d.c. <a id="page_16" class="page">Page 16, 1 Fundamental ideas</a>one. It has, however, other even more useful properties. If the ratio of turns of wire on the secondary to those on the primary winding is <i>n:</i> 1, then <i>V<sub>oul</sub> &#x003D; nV<sub>in</sub>,</i> where <i>V<sub>in</sub></i> is the voltage applied to the input and <i>V<sub>out</sub></i> that produced at the output. Thus a transformer with a turns ratio greater than unity can be used to increase voltages (although the available current is reduced in proportion to the turns ratio). With a turns ratio of less than unity, a step-down transformer reduces the available voltage but the available current is increased. One further useful property of transformers is that, if a load of impedance <i>Z<sub>L</sub></i> is connected across the secondary, the primary behaves as though it had an impedance of <i>Z<sub>L</sub>/n<sup>2</sup>.</i> The device therefore transforms voltage, current, or impedance. Very great use is made of transformers at low frequencies for the distribution of mains power and in the fabrication of power supplies for experiments and for the instruments themselves. For these low-frequency purposes, transformers almost always have a high-permeability core coupling the two windings. But there are many other uses of transformers in interfacing experimental systems to the signal conditioning equipment. It is often the impedance-matching capability of the transformer that is of greatest value in these applications. For low-power, high-frequency purposes, the transformer may be air-cored.</p>

<aside class="abc" style="margin-top:-20em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.22">
<img src="../images/page_16_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.22</b> A transformer, showing how the ratio of the number of turns of wire in the primary and secondary windings determines the relationships between input and output voltages, currents, and impedances</p></figure></aside>
</section>
<section epub:type="chapter" id="sec_1.5">
<h2 class="h2">1.5<span class="space">	&#160;</span> 
Diodes, transistors, and integrated circuits</h2>


<p class="noindent">This section deals with a variety of <i>non-linear</i> electronic devices that are essential elements in the signal processing and conditioning offered by modern instrumentation. Their functions range from the simplest two-terminal component to devices that offer full-scale computing power. Electronics has its origins in investigating the behaviour of electrons in near-vacuum conditions, and <i>thermionic vacuum tubes,</i> or &#x2018;valves&#x2019;, were formerly used almost universally in electronic equipment. Semiconductor devices have, however, virtually replaced vacuum tubes except for a very few specialized applications, and we shall discuss only semiconductors here. They depend for their action on a semiconducting material, often silicon or germanium, <i>doped</i> with a small amount of impurity that provides excess electrons (n-type material) or a deficiency of them (p-type material). Our objective is to see what these devices do rather than to explain how they work, and the reader is referred to any one of a number of excellent texts that show why junctions of p- and n-doped semiconductors behave as they do.</p>


<aside class="abc" style="margin-top:1.6em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.23">
<img src="../images/page_16_2.jpg" alt="images" style="margin-left:1em"/>

<p class="aside"><b>Fig. 1.23</b> The diode</p></figure></aside>

<h3 class="h3">Diodes</h3>
<p class="noindent">The simplest of all the devices is the <i>diode,</i> and it consists of a single junction between p- and n- type material. One of the most important properties of the diode is that, expressed crudely, it allows electrons to pass from the n-side to the p-side, but not vice versa. Thus, if the n-side is made negative and the p-side is made positive, a current will flow, but if the polarity is reversed, the diode will behave almost as an insulator. If the diode is connected to a d.c. <a id="page_17" class="page">Page 17, 1 Fundamental ideas</a>
voltage such that it is conducting, the negative terminal is the <i>cathode</i> and the positive terminal the <i>anode.</i> Of course, a real diode is neither a perfect insulator nor a perfect conductor, as the <i>I-V</i> graph shows (<a href="#fig_1.24">Fig. 1.24</a>). For a silicon diode, conduction in the forward direction does not start until a threshold of about 0.6 volts has been passed, but beyond that it becomes a rather good conductor. The diode therefore behaves as a one-way element. One use is in converting a.c. into d.c. signals. If an a.c. signal is connected to a diode, only the positive half cycles will be conducted. A series of positive bursts of current or voltage appear at the output. A smooth d.c. level can then be obtained by connecting this output to a low-pass <i>RC</i> fdter. This kind of application of the diode is called <i>rectification</i> and the diode is being used as a <i>rectifier.</i> Rectification circuits are used very widely in power supplies for instrumentation as well in converting a.c. signals that originate in the experiments, or that have been introduced in the instrument, into d.c. voltages for display and recording. Another use of diodes that makes use of their one-way conductivity is as high-speed <i>switches,</i> in which a <i>forward</i> or <i>reverse bias</i> is applied in order to make the diode allow or prevent the passage of other signals.</p>

<aside class="abc" style="margin-top:-22em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.24">
<img src="../images/page_17_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.24</b> Diode characteristics: relation between current and applied voltage in forward and reverse directions</p></figure></aside>


<p class="indent3-ka">A rather different kind of application of diodes, but nevertheless an important one, is as a <i>voltage regulator.</i> The diagram showing the diode characteristics indicates that, if the diode is reverse biased, at some voltage it begins to conduct again. <i>Zener diodes</i> are manufactured so that this breakdown occurs very sharply and at a closely specified voltage. After breakdown, the current increases very rapidly for very small increases in voltage. If a voltage is applied via a resistor to a zener diode, then the voltage across the diode remains virtually constant regardless of the applied voltage so long as it exceeds the breakdown voltage. The zener diode can therefore provide a nearly constant voltage to an experiment or a piece of instrumentation regardless of variations in the supply voltage or in the load imposed (changes that are, perhaps, a consequence of altering current demands of the equipment), at least within certain limits. This property is clearly very useful, and is frequently used either directly or as part of a rather more sophisticated stabilizing circuit.</p>



<aside class="abc" style="margin-top:-22em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.25">
<img src="../images/page_17_2.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.25</b> The diode as a rectifier to convert a.c. into d.c. The lower diagram shows how the half-wave rectified output can be &#x2018;smoothed&#x2019; by an <i>R-C</i> filter</p></figure></aside>


<h3 class="h3">Transistors</h3>
<p class="noindent">The next device to be considered is the three-terminal <i>transistor.</i> Two main types of transistor exist, the <i>junction transistor</i> and the <i>field effect transistor (FET).</i> Diagrams for each are shown in<a href="#fig_1.27"> Fig. 1.27</a>; the three terminals are the <i>emitter, base,</i> and <i>collector</i> in the junction transistor and the <i>source, drain,</i> and <i>gate</i> in the FET. The way in which they are used is similar, and we shall confine our discussion for the time being to the junction transistor. If the semiconductor materials are arranged so that the emitter and collector regions are constructed of n-type material, and the base of p-type (<a href="#page_16">see p. 16</a>), the device is an <i>npn transistor,</i> the alternative construction is a <i>pnp transistor.</i></p>



<aside class="abc" style="margin-top:-12em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.26">
<img src="../images/page_17_3.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.26</b> The zener diode</p></figure></aside>



<p class="indent3-ka"><a id="page_18" class="page">Page 18, 1 Fundamental ideas</a>&#160;&#160;&#160;&#160; The basic function of the transistor is to act as an <i>amplifier.</i> If the emitter and collector are connected to a voltage supply, the transistor can pass a current. To pass the emitter-collector current, it is necessary that a small current is also injected into the base. However, the emitter-collector current is strongly dependent on the base current, and changes by large amounts for relatively small changes in base current. A change in base current &#x03B4;<i>i</i> appears at the collector as a change <i>A.&#x03B4;i,</i> where <i>A</i> is the amplification or <i>gain</i> of the transistor, and may be much greater than unity (values of several hundred are not uncommon). We thus have the basis of a <i>current amplifier</i> which permits a small current from some experimental system to be converted to a larger current that can do something &#x2018;useful&#x2019;. In one familiar example from the domestic field, the small current might come from the head of a tape recorder that converts the magnetic information on the tape to an a.c. signal, and the large amplified current might drive the coils of a loudspeaker so that we can hear the music. The transistor can also be used as a <i>voltage amplifier,</i> as shown in <a href="#fig_1.28">Fig. 1.28</a>, because applying a voltage change to an input resistor connected in series with the base produces a change in base current. The collector current can, in turn, be passed through a load resistor, and the amplified changes in the resistor will develop amplified voltages across it in accordance with Ohm&#x2019;s law. The FET works in a very similar way; one key difference is that it is the input voltage, rather than an input current, that directly leads to the change in output current. As a result, the input impedance of an FET is very high (the impedance at the base of a junction transistor is low), and this may be a very desirable quality if the amplifier is not to &#x2018;load&#x2019; the signal source and thus alter its characteristics.</p>





<aside class="abc" style="margin-top:-30em;" epub:type="sidebar">
<figure class="fig1" id="fig_1.27">
<img src="../images/page_18_1.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1 .27</b> The symbols for</p>
<p class="aside" style="margin-top:-1em">(a)	a bipolar transistor; and</p>
<p class="aside" style="margin-top:-1em">(b)	a field-effect transistor</p></figure>

<figure class="fig1" id="fig_1.28">
<img src="../images/page_18_2.jpg" alt="images" style="margin-left:1em"/>
<p class="aside"><b>Fig. 1.28</b> A bipolar transistor used as an amplifier. The input current to the base appears multiplied by the gain factor <i>A</i> at the collector. The circuit is that of a current amplifier, but voltage amplification can be achieved by allowing the output collector current to develop a voltage across the load <i>R<sub>L</sub></i></p>
<p class="noindent"> </p></figure>

<figure class="fig1" id="fig_1.29">
<img src="../images/page_18_3.jpg" alt="images" style="margin-left:2em"/>
<p class="aside"><b>Fig. 1.29.</b> An oscillator. The <i>R-C</i> circuits shift the phase of the output to be in-phase with the input at the oscillation frequency</p></figure></aside>




<p class="noindent">To obtain higher gain, many transistor amplifiers can be connected together in suitable ways. Many other important circuit functions can also be obtained. Sometimes it is necessary to generate sine, square, or other shaped waves at a specified frequency in order to drive some experiment. One or more transistors can be configured to act as an <i>oscillator</i> (<a href="#fig_1.29">Fig. 1.29</a>). Some of the amplified output is fed back to the input in the correct phase so that it is also amplified. If the connections from output back to input are made <i>via</i> frequency-dependent circuitry (see<a href="#sec_1.4"> Section 1.4</a>), then the complete device may produce sustained oscillations at the desired frequency. The transistor may also be used as a <i>switch,</i> because with suitable base current or gate voltage, the transistor can be made to pass negligible current, while with other currents or voltages, the transistor can be made to appear as a fairly low resistance. Under these conditions, the transistor is said to be <i>saturated,</i> and the output current is no longer much dependent on the exact input conditions. The transistor is either &#x2018;off or &#x2018;on&#x2019;. The currents it passes can be much larger than those that control its switching function. Applications of this behaviour are very numerous, and include switching on lamps, heaters, or motors. Perhaps more important than any of these uses, however, is the possibility of using the switching properties in <i>digital logic circuits.</i> Such circuits are at the heart of computers, and will be described in the next section.</p>



<p class="noindent"><a id="page_19" class="page">Page 19, 1 Fundamental ideas</a></p>
<h3 class="h3">Integrated circuits</h3>
<p class="noindent">Nothing has been said about the detailed design of circuits incorporating transistors. One reason for this omission is the present-day widespread availability of <i>integrated circuits</i> that can perform almost any desired function with the addition of a few external components. Within the integrated circuits are anything between two and tens of thousands of transistors, together with the associated diodes, resistors and capacitors, often all laid out on the same small silicon wafer or chip. The internal design and component values have been optimized, so that the user does not have to worry about these aspects. Mass-produced integrated circuits are exceptionally cheap, and, except for special purposes, it is rare for the experimental chemist or physicist to have to use discrete transistors at all. The design and construction of instrumentation has thus altered from one in which the detailed electronics have to be considered, and is now more akin to fixing together a series of building blocks to perform the required operations.</p>
<aside class="abc" style="margin-top:-1em" epub:type="sidebar">
<figure class="fig1" id="fig_1.30">
<img src="../images/page_19_1.jpg" alt="images" style="margin-left:3em"/>
<p class="aside"><b>Fig. 1.30</b> Symbol for a NAND gate</p></figure></aside>

<p class="indent3-ka">There are two families of integrated circuits: <i>analogue</i> and <i>digital.</i> The analogue circuits are based on amplifiers. A continuum of input and output voltages or currents is possible (although the relationship of output to input is not necessarily linear). Amplification itself is obviously a central application of these integrated circuits, but many are designed with more esoteric specific uses in mind, including fabricating oscillators, comparators, multipliers, and so on. We shall examine a special use of amplifiers as <i>operational amplifiers</i> in <a href="Chapter04.xhtml">Chapter 4</a>, and defer further discussion of linear and analogue circuits until then. The digital circuits are based on the switching capability of the transistor, and respond only to whether a signal is present beyond a threshold level. The input levels in many of these devices are thought of as logical &#x2018;0&#x2019; or &#x2018;1&#x2019;, and the output is also usually just &#x2018;0&#x2019; or &#x2018;1&#x2019; as well. A typical simple integrated logic element is the <i>NAND gate</i> (the word &#x2018;NAND&#x2019; stands for &#x2018;Not AND&#x2019;). The <i>truth table</i> for a NAND gate is shown in<a href="#fig_1.31"> Fig. 1.31</a>. Other important logic gates (many of which can be built up just from NAND gates) include those with <i>AND</i> and <i>OR</i> functions. These simple elements can then be used to construct more complex logical functions and are at the heart of counters, dividers, memories, and much more besides. Of course, more complicated integrated circuits are fabricated that combine many of these functions on single chips, and they reach a very high level of sophistication in the <i>central processing unit (CPU)</i> of even the most humble computer. The computer itself is beginning to replace the individual integrated circuits in many instrumentation applications, because it can be <i>programmed</i> to perform a multitude of functions that would previously have required dedicated integrated circuits or large arrays of logic chips for which the design would have demanded great thought and effort. At least at the level of a CPU and associated memory and peripheral integrated circuits, it is not unreasonable to provide a small programmed computer to replace many of the specialized devices. Of course, it is a far cry from the days when a computer itself would require rooms full of vacuum tubes, but then that is the kind of progress that has made exciting developments in chemical instrumentation possible.</p>

<aside class="abc" style="margin-top:-25em;" epub:type="sidebar">
<table id="fig_1.31" style="margin-left:4em">
<tr><td style="border-bottom:1pt solid black;width:20%">A</td>	<td style="border-bottom:1pt solid black;border-right:1pt solid black;width:20%">B</td>	<td style="border-bottom:1pt solid black;width:20%">0</td></tr>
<tr><td>0</td>	<td style="border-right:1pt solid black">0</td>	<td>1</td></tr>
<tr><td>0</td>	<td style="border-right:1pt solid black">1</td>	<td>1</td></tr>
<tr><td>1</td>	<td style="border-right:1pt solid black">0</td>	<td>1</td></tr>
<tr><td>11</td>	<td style="border-right:1pt solid black">1</td>	<td>0</td></tr>
</table>

<p class="aside"><b>Fig. 1.31</b> Truth table for the NAND gate. If both A and B inputs are at logical &#x2018;1&#x2019; levels, the output is &#x2018;O&#x2019;. For all other input combinations, the table shows that the output is &#x2018;1&#x2019; That is, the output is affected only if A <i>and</i> B are &#x2018;1&#x2019;; because this is a NAND gate, the output is <i>negated</i> (ie, &#x2018;0&#x2019; rather than &#x2018;1&#x2019;)</p>

</aside>
</section>
</section>
</div>
</body>
</html>
