<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en" lang="en">
<head>
<meta charset="utf-8"/>
<title>Computers in Chemistry</title>
<link rel="stylesheet" href="../styles/stylesheet.css" type="text/css"/>
</head>
<body>
<section epub:type="chapter" id="ch02">
<a id="page_12" class="page" style="width:70%;">Page 12, Chapter 2 The Hardware</a>
<h1 class="main">2<span class="space">&#160;</span>The hardware</h1>
<section epub:type="chapter" id="ch2.1">
<h2 class="h2">2.1 What is a computer?</h2>
<p class="banoindent">This is actually a very difficult question to answer. A computer is different things to different people. To most people, a computer is a beige box sitting on a desk; a computational chemist may think of it as an anonymous series of cabinets at the other end of a cable; to others it is a black piece of plastic containing silicon. In actual fact, a computer is all of these things, and more. In <a href="Chapter01.xhtml">Chapter 1</a> some background to the history of computers was presented, and some insight into how computers developed was given. Also in that chapter was a brief introduction to digital electronics: the building blocks of computers. Now we delve into what a computer is physically made up of.</p>
<h3 class="h3"><b>Elements of a computer</b></h3>
<p class="banoindent">The basic elements of a computer haven&#x2019;t changed since Babbage&#x2019;s time, although they have certainly changed in their implementation. <a href="#fig2.1">Fig. 2.1</a> shows a very simplified block diagram of a computer. The main elements are a <i>central processor unit</i> (<i>CPU</i>) that does the work,<i> memory</i> to store results and the controlling program,<i> input</i> and<i> output</i> (<i>I/O</i>) devices to communicate with the outside world, and<i> buses</i> to provide the communication between the various elements. In the following few sections we will look at each of these in more detail.</p>
<figure class="image" id="fig2.1">
<img src="../images/img11.jpg" alt="images"/>
<figcaption style="text-align:center;">Fig. 2.1 The basic elements of a computer</figcaption></figure>
<p class="noindent"><a id="page_13" class="page" style="width:70%;">Page 13, Chapter 2 The Hardware</a></p>
<h3 class="h3"><b>Memory and buses</b></h3>
<p class="banoindent">It may seem strange to start by describing memory, rather than the CPU, but it is memory that makes a computer unique. Memory can be thought of as a series of locations into which information can be put; with each of these locations having an address by which it is known. The specific memory location required by the CPU is controlled by the<i> address bus</i>, and the data which is read or written to that location is on the<i> data bus</i>; the type of access (<i>i.e.</i> read or write) is controlled by the<i> control bus.</i></p>
<p class="baindent">There are a number of different types of memory commonly in use.<i> Readonly memory</i> (<i>ROM</i>) is memory whose contents are permanent,<i> i.e.</i> it can only be read, and not (usually) written to, and the contents do not disappear when power is removed. Typically ROM is used to store permanent programs such as those required when the computer is first switched on. <i>RAM</i> (<i>random access memory</i>) is memory that can be both read from and written to. In an application where the computer is a dedicated controller, the amount of RAM need only be very small as it is only used for storing data as it is manipulated by the CPU. However, in most modern computer systems, the vast bulk of memory is RAM; RAM in this case holds both data that is being processed by the CPU and the program code controlling that processing. Unfortunately RAM, by its very nature, is volatile, and the contents of RAM are lost when power is removed from the memory.</p>
<aside class="abc" style="margin-top:2.8em;" epub:type="sidebar">
<p class="noindent2" style="margin-left:1em;">The unit multipliers used in computing are slightly different from those specified for SI units&#x0021; The basic unit is a &#x2018;<i>bit</i>&#x2019; (standing for<i> binary digit),</i> 8 bits is called a &#x2018;<i>byte</i>&#x2019; with 4 bits often being called a &#x2018;<i>nibble</i>&#x2019;. The abbreviation for bit is &#x2018;b&#x2019; and that for byte is &#x2018;B&#x2019;</p>
<p></p>
<p class="noindent2" style="margin-left:1em;">2<sup>10</sup> &#x003D; 1024 &#x003D; lk<br/> 2<sup>20</sup> &#x003D; 1024 k &#x003D; 1M (&#x2018;mega&#x2019;)<br/> 2<sup>30</sup> &#x003D; 1024 M &#x003D; 1 G (&#x2018;giga&#x2019;) 2<sup>40</sup> &#x003D; 1024 G &#x003D; 1 T (&#x2018;tera&#x2019;)</p>
<p></p>
<p class="noindent2" style="margin-left:1em;">Hence 2MB is 2048kB or 2,097,152 bytes.</p>
</aside>
<p class="baindent">The maximum amount of memory in a computer is determined largely by the size of the buses. In early CPUs, the address bus was 16-bits, or less, wide (<i>i.e.</i> there were 16 lines making up the bus) meaning that a maximum of 65,535 (64k) locations could be accessed, whilst the data bus was at most 8-bits wide. The processor used in the Acorn BBC Micro (the Motorola 6502) falls into this class. However, the first IBM PCs used either an 8086 or 8088 CPU (both from Intel) which have a 20-bit wide address bus with either a 16- or 8-bit wide data bus respectively. Consequently the 8086 can address up to 1,048,576 (1024k) locations. This address space seemed very large at the time, especially when you consider that memory was probably the largest cost single item in a computer, but it is dwarfed by the massive address spaces available in modern computers. The most recent Intel processor (the PII) has a 32-bit address bus giving an address space of 4Gb, with many of the CPUs used in larger machines capable of addressing over 1Tb of memory; no doubt in the future, these memory sizes will seem small.</p>
<p class="baindent">There are other, more specialised, types of memory commonly found in computers that should be mentioned here. One of the features of memory is that for most of the time, it is quiescent: the CPU can usually only address one memory location at a time, and in general it will access the memory sequentially. So, rather than use expensive high speed memory for all the memory in the computer, lower cost slower memory is mostly used, with a small amount of high speed memory used to act as temporary store for the data that the CPU is currently accessing. This temporary memory is called <i>cache.</i> Since it is accessed at very high speeds, cache is usually located very close to the CPU, both logically and physically, indeed most CPUs now come with cache located on the same chip as the CPU. The control circuitry for the cache normally loads large sections from main memory into cache each time that a new memory area is accessed; the principle behind this behaviour is that on average, when the CPU accesses a memory location, it <a id="page_14" class="page">Page 14, Chapter 2 The Hardware</a>will access the next location in sequence. When the cache is full, the oldest unused data is written back to main memory and the area re-used. The detailed operation of cache is beyond the scope of this book, but it will become evident in later chapters that the amount and organisation of any cache is of critical importance in designing efficient programs.</p>
<p class="baindent">There are also types of bus other than the basic ones mentioned so far. Of most importance to us here is the fact that some processors have a separate bus dedicated to I/O operations. This I/O bus is generally slower than the main buses (since I/O devices are generally slower than the CPU), and is not as large. For instance, the 8086 series of Intel CPUs have a 16-bit wide I/O address bus allowing up to 64k locations to be addressed (there is no separate I/O data bus). The I/O bus is used to communicate with devices attached to the outside world, such as video, serial lines, printers and, very importantly for our purposes, experiments.</p>
<h3 class="h3"><b>The central processor unit</b></h3>
<p class="banoindent">The central processor unit, as its name suggests, is the key element of a computer. The CPU contains an<i> arithmetic-logical unit</i> (<i>ALU</i>) along with its associated registers (or storage areas) and control unit (<a href="#fig2.2">Fig. 2.2</a>). The ALU performs all the arithmetic and logical operations of the computer with special registers, called accumulators, providing the input data for the ALU and receiving the results of its operations. In addition to the arithmetic and logic functions, the ALU provides the basic<i> shift</i> and<i> rotate</i> functions. These functions, illustrated in <a href="#fig2.3">Fig. 2.3</a>, move the results of the ALU left or right by one or more positions.</p>
<figure class="image" id="fig2.2">
<img src="../images/img12.jpg" alt="images"/>
<figcaption>
<aside class="abc1" style="margin-top:-17em;" epub:type="sidebar">
Fig. 2.2 Block diagram of a typical CPU</aside></figcaption>
</figure>
<p class="baindent">The<i> status</i> or<i> condition code</i> register is a special register that stores the <a id="page_15" class="page">Page 15, Chapter 2 The Hardware</a>internal status of the ALU. This register consists of a number of single-bit stores called<i> flags.</i> The flags reflect conditions within the ALU such as when the result of an operation is zero, or when an arithmetic operation gives a value too big to hold in a register (<i>overflow</i>). The flags are important as it is often the status of these flags that affect the program flow and allows the CPU to make &#x2018;decisions&#x2019; on which instruction to execute next.</p>

<aside class="abc" style="margin-top:-7.05em;" epub:type="sidebar">
<figure id="fig2.3" style="margin-left:1em;">
<img src="../images/img13.jpg" alt="images"/>
<figcaption>Fig. 2.3 The shift and rotate functions</figcaption></figure></aside>
<p class="baindent">Another very important register is the<i> program counter</i> (PC). This register is present in all CPUs and is fundamental to program execution as it contains the address in memory of the next instruction to be executed. Whenever the CPU needs another instruction, it places the contents of the PC onto the address bus, then reads the contents of the data bus into the<i> instruction decoder</i> to be processed.</p>
<p class="baindent">The<i> stack pointer</i> (SP) contains the address of the top of the<i> stack.</i> The stack is crucial in the operation of the computer since it is the mechanism by which the processor &#x2018;remembers&#x2019; its state when it needs to break off from one operation and start another, then return to the original process. Formally the stack is a LIFO (last-in, first-out) structure. It can be visualised as being like a pile of papers: the first item to be put on the pile will always be at the bottom and will be the last to be retrieved, conversely the most recent item put on the top of the pile will be the first to be retrieved.</p>
<p class="baindent">The final element of <a href="#fig2.2">Fig. 2.2</a> to be discussed is the<i> index register.</i> This register is used in calculating addresses of data that are needed by the CPU. Typically, this register contains a displacement, which is automatically added to a base value when forming an address. In this way, data within a block can easily be accessed; it is the basic operation involved in creating arrays. Not all CPUs have dedicated index registers and it is often the case that the general-purpose registers can be used instead.</p>
<h3 class="h3"><b>Input/Output</b></h3>
<p class="banoindent">Input and output, although not fundamental to the operation of the computer, are really its<i> raison d&#x2019;&#x00EA;tre</i>! There are many types of I/O device: as I sit here typing I am using a keyboard, many of the formatting operations require the use of a mouse, I am looking at the results on a VDU, and the text I am working on is stored on a hard disk. But by and large, we do not have to concern ourselves with how these complex operations are achieved, just some of the fundamentals. I/O devices can be thought of as special memory locations: you can store information at I/O locations and read data from them like any other part of memory. The only difference is that the bits and bytes stored in these locations have some discernible affect on the outside world, and the outside world can affect what information is read from those locations. We will investigate this much further in <a href="Chapter03.xhtml">Chapter 3</a>.</p>
</section>
<section epub:type="chapter" id="ch2.2">
<h2 class="h2"><b>2.2 CPU architectures</b></h2>
<p class="banoindent">The architecture of a CPU is the layout of the registers, ALUs, and control circuitry within the chip. Each type of CPU has its own architecture, and there are generally as many architectures as there are types of CPU. However, the basic way of describing CPU architecture is the complexity of its instruction set. The instruction set of a CPU are the basic operations that it understands: these operations are things such as &#x2018;store register A in the<a id="page_16" class="page">Page 16, Chapter 2 The Hardware</a></p>
<aside class="abc" style="margin-top:0.8em;" epub:type="sidebar">
<p class="noindent2" style="margin-left:1em;">Clock cycles: we saw in <a href="Chapter01.xhtml">Chapter 1</a> how a<i> clock</i> is used to control how some logic elements, such as flip-flops, work. The same principle is used in larger, more complex, circuits. The clock in a computer is the basic signal that initiates transfers between registers, memory and so on and is the basic synchronising source.</p></aside>
<p class="banoindent">memory location pointed to by register B&#x2019; or &#x2018;add register A to the contents of register C&#x2019; Some instructions are very basic, such as the store or add instructions; others can be much more complex, such as multiply or moving a block of memory. The complexity of an instruction has a large impact on the amount of time it takes the CPU to execute it; for example, the add instruction takes only 3 or 4 clock cycles to execute, whereas a multiply instruction may take over 150 clock cycles. Things become even worse when complex addressing modes are used. However, analysis of programs shows that these complex instructions are actually used very infrequently, but they contribute a very large part to the electronic complexity of the chip. Consequently, a series of CPUs were developed in which the instruction set was restricted to only the very basic operations, the so-called<i> Reduced Instruction Set Computer</i> or<i> RISC</i> processor. One of the main benefits of a reduction in the complexity of the instruction set is the ability to<i> pipeline</i> instructions. This ability arises from being able to ensure that each instruction in a RISC processor takes the same amount of time to execute, and that the execution of every instruction proceeds in set of well-defined steps. Hence it is possible to start processing the next instruction in sequence while the current instruction is part way through being executed. By arranging for there to be a number of these<i> pipelines</i>, it can be possible that the processor will finish an instruction on almost every clock cycle, so vastly speeding up the throughput of the system. Obviously, the presence of branch instructions in the code will mean that the pipelines will break occasionally, but much effort has been put into developing<i> branch-prediction</i> and<i> look-ahead</i> units in CPUs to reduce their significance.</p>
<p class="baindent">Another consequence of using a RISC architecture is that caching becomes very efficient; the combination of branch prediction and look ahead vastly increases the efficacy of cache. Indeed all RISC CPUs have on-chip caches of varying size.</p>
<p class="baindent">This is not to say, however, that<i> complex instruction set computer</i>, or <i>CISC</i>, processors no longer exist. There are many fields where CISC architectures are important, especially where a computer primarily does one task. Examples of such are database servers: take for example a large company with a stock database, there may be many millions of database queries and modifications (or &#x2018;transactions&#x2019;) every second, so to attain the best possible throughput of transactions, highly specialised machine instructions are designed into the processor which aid in the processing of transactions.</p>
<p class="baindent">Not all manufacturers have polarised into RISC or CISC. Some, such as Intel with their Pentium series, have developed hybrid processors. These are processors that are by and large RISC, but have a number of CISC type instructions. In the Pentium, especially the MMX series, these CISC instructions are present to help the processor, and programmer, to perform multi-media operations (such as displaying and moving images). The problems of introducing these complex instructions into a RISC architecture are seen to be outweighed by the advantages that the presence of these instructions bring.</p>
<p class="baindent">From a computational point of view, the most important CISC instructions are those that involve floating-point (FP) operations. Many of the time-consuming parts of a scientific program are those that perform calculations <a id="page_17" class="page">Page 17, Chapter 2 The Hardware</a>on real numbers (as opposed to integers). These FP operations can be performed using integer style instructions with some pre- and postprocessing to make sure the calculations are accurate. However, the operations are much faster if there is a dedicated set of instructions to perform them. Indeed there are often FP<i> co-processors</i> available to provide this functionality for CPUs which do not have it built-in. Nevertheless, even dedicated FP processors can take some time to calculate complex functions such as logarithms or square roots and consequently the main CPU may spend long periods waiting for the results of the calculations. Modern systems get around this idle time by arranging for the FP unit to be able to work independently and for the main CPU to start a calculation in the FP unit and then resume other operations until the FP result is ready. In this way the overall performance of the CPU is maintained.</p>
</section>
<section epub:type="chapter" id="ch2.3">
<h2 class="h2"><b>2.3 Classes of computers</b></h2>
<p class="banoindent">Historically computers have been divided into different classes according to their power, capabilities and designed use. These classifications are, going from smallest to largest: micro-controllers, microprocessors, microcomputers (PCs), minicomputers, mainframes and supercomputers. Over recent years, a further class, workstations, has been added between microcomputers and minicomputers. However these historical boundaries have been getting very fuzzy recently. It is now possible to have what are essentially minicomputers with the power and architecture of supercomputers; many PCs are now called &#x2018;workstations&#x2019; and have considerably more power than mainframes of only a decade ago. Nevertheless, it is still instructive to examine what are the similarities and differences between these various classes of computers.</p>
<h3 class="h3"><b>Micro-controllers</b></h3>
<p class="banoindent">These days micro-controllers are everywhere: in your washing machine, car, domestic heating system. They can be found in almost any piece of electrical equipment that has more than a simple on-off function&#x0021; In general microcontrollers are very simple computers that perform one dedicated task, this task being built into the CPU at manufacture and usually not alterable. They are computers though and possess a CPU, memory and I/O functions, albeit at a much lower level than a &#x2018;normal&#x2019; computer &#x2013; the CPU may only be 4-bit processor and there may only be a few bytes of RAM &#x2013; but it does follow a predetermined sequence of instructions, and the flow of the program is controlled by external input.</p>
<h3 class="h3"><b>Microprocessors</b></h3>
<p class="banoindent">This is one of the terms in computing which has acquired a multiplicity of meanings. The original concept of a microprocessor is the CPU in a computer, although now it has also come to mean a computer with limited capabilities or one that is used to implement complex control functions. Examples of microprocessor controlled equipment are printers, modems and calculators &#x2013; the CPUs may be quite powerful and fast, and often have substantial amounts of RAM, but they are still dedicated to doing one task, but that task can be changed, or reprogrammed, after manufacture.<a id="page_18" class="page">Page 18, Chapter 2 The Hardware</a></p>
<h3 class="h3"><b>Microcomputers</b></h3>
<aside class="abc" style="margin-top:1em;" epub:type="sidebar">
<p class="noindent2" style="margin-left:1em;">The term &#x2018;PC&#x2019; will be used for all IBM PC compatible computers. Similarly, the term &#x2018;Mac&#x2019; will be used for the Apple Macintosh and its clones.</p></aside>
<p class="banoindent">Microprocessor based computers that are not dedicated to specific tasks are often called microcomputers. This class of computers is by far the biggest in terms of numbers. It includes all the &#x2018;home&#x2019; computers as well as the vast majority of desktop and &#x2018;business&#x2019; computers. The largest group of microcomputers are the Personal Computers (PCs) manufactured originally by IBM and Apple Corp., although now imitated (or cloned) by many other manufacturers.</p>
<p class="baindent">Microcomputers are characterised by their ability to perform different tasks according to the program (or software) they are currently executing, and the ability to rapidly change between different programs without the need to change any embedded software. A further feature of microcomputers, which is not usually found in microprocessors, is some form of<i> mass storage</i> device. These devices enable the long-term storage of large quantities of information; this information may be either data generated by the machine, or programs for it to execute.</p>
<p class="baindent">This ability to switch between different tasks has to be controlled in some way, and most microcomputers have some form of supervisory program performing this r&#x00F4;le called the<i> operating system</i> (OS). There are many different operating systems available for microcomputers; you will most likely come across MS-DOS or WinNT for PCs or MacOS for Macs, but there are also OS&#x2019;s such as Linux for both PCs and Macs or FreeBSD or O/S2 for PCs that you may encounter. There will no doubt be many more OS&#x2019;s written in the future, but it is likely that they will, at least in the near future, be based in some way on one of the ones mentioned above&#x0021;</p>
<p class="baindent">As indicated before, microcomputers are the most common class of computers, indeed most of the software and hardware described in this book will be based on microcomputers, and more specifically PCs. This does not mean that it is not applicable to other machines, just that it is the most likely scenario you will come across.</p>
<h3 class="h3"><b>Workstations</b></h3>
<p class="banoindent">Some consider workstations to be &#x2018;grown-up&#x2019; versions of microcomputers; others consider them to be &#x2018;stripped-down&#x2019; minicomputers&#x0021; Whichever is the case, workstations can generally be said to be powerful personal computers. Often they have very good graphics capabilities, a large amount of memory and are connected to a network. The software run on workstations tends to be some form of<i> multitasking</i> OS, enabling the machine to run more than one process at a time; the individual processes are often graphically based and are controlled through a<i> graphical user interface</i> (GUI). However, the term workstation is more of a description of the intended use of the machine than an absolute name for a type of machine: one person may use a type and model of machine as a personal computer/workstation, whilst another may use the same machine as a server/minicomputer.</p>
<p class="baindent">At the low-end of the workstation range are the powerful PCs and Macs, whilst at the high-end are powerful machines from manufacturers such as Silicon Graphics, Sun, Hewlett Packard, IBM and so on. In-between is a large range of machines from a number of manufacturers.<a id="page_19" class="page">Page 19, Chapter 2 The Hardware</a></p>
<h3 class="h3"><b>Minicomputers</b></h3>
<p class="banoindent">Most of the &#x2018;large&#x2019; computers chemists encounter will come in this class. Minicomputers are medium sized multi-user machines that are almost invariably housed in a single box or cabinet. They will have one or more CPUs working together and will have moderate amounts of both RAM and mass storage. The OS will probably be some form of<i> Unix</i>, but might also be VMS or some other proprietary OS. Often these computers are<i> servers</i>, that is, machines that provide information for other computers to use.</p>
<p class="baindent">Minicomputers will usually have compilers or interpreters for a number of different high-level languages to enable the user to write their own programs, as well as many other utilities for performing common tasks. In general the user will never need to know, or care, what sort of CPU the machine uses, as they do not need to interact directly with the hardware. Typically the manufacturers of these machines are Sun, Silicon Graphics or Hewlett Packard amongst others.</p>
<h3 class="h3"><b>Mainframes</b></h3>
<p class="banoindent">The term &#x2018;mainframe&#x2019; comes from the days when computers occupied whole rooms and there were many different cabinets, or frames, containing electronics that performed discrete functions; the &#x2018;mainframe&#x2019; was the set of cabinets containing the central processor along with the main memory of these physically large machines. Now the term &#x2018;mainframe&#x2019; refers to any large computer, although more recently it has come to be used mainly for very large servers and for machines that the user doesn&#x2019;t interact directly with. A mainframe will probably have multiple processors or consist of a <i>cluster</i> of tightly coupled machines; the CPUs will be proprietary and have a CISC architecture (although RISC based mainframes are starting to appear). The whole machine, especially the CPU architecture, will often be tuned for a specific application, such as a database server.</p>
<h3 class="h3"><b>Supercomputers</b></h3>
<p class="banoindent">Supercomputers are the glamour end of the market&#x0021; These machines are very large often with hundreds of linked high-speed processors and vast quantities of memory. They are typically used for very large calculations in meteorology, materials science or astronomy and are tuned for doing large floating point calculations. The calculations are often performed across many of the processors at once: such<i> parallel processing</i> is very efficient when large arrays are being manipulated. By far the most famous supercomputers are those of Cray Research, although other manufacturers such as Silicon Graphics, CDC, Fujitsu, NEC, and Hitachi also produce supercomputer class machines.</p>
<p class="baindent">Surprisingly, supercomputers are often interactive machines and appear to the user to be like a powerful minicomputer. However interactive use is not efficient on these machines, so they are often linked with a minicomputer <i>front-end</i> which provides the basic user interface, and only the large computing jobs are actually run on the supercomputer.</p>
</section>
<section epub:type="chapter" id="ch2.4">
<a id="page_20" class="page" style="width:70%;">Page 20, Chapter 2 The Hardware</a>
<h2 class="h2"><b>2.4 Examples of hardware</b></h2>
<p class="banoindent">As examples of the internal architecture of different computers we will take a detailed look at two very different machines. The first is, inevitably, the IBM PC compatible, the second is the Silicon Graphics Origin2000 series. The IBM PC is a personal microcomputer and can be found in almost any laboratory: it is used to control experiments, as a database, a word processor, a terminal and much, much more; the SG Origin is a minicomputer with many supercomputer features and in general it is a department or campus based machine. These particular machines have been chosen for no other reason than that they are the ones that the author is most familiar with.
</p>
<figure class="image" id="fig2.4">
<img src="../images/img18.jpg" alt="images"/>
<figcaption>
<aside class="abc1" style="margin-top:-20em;" epub:type="sidebar">
Fig. 2.4 Simplified block diagram of a PC</aside></figcaption>
</figure>
<h3 class="h3"><b>The IBM PC</b></h3>
<p class="banoindent">A simplified block diagram of the internal structure of a PC is shown in <a href="#fig2.4">Fig. 2.4</a>. As can be seen it is considerably more complicated than that shown in <a href="#fig2.1">Fig. 2.1</a>. The system shown here happens to be a dual CPU one; however that in itself is not important, as the presence of a second CPU has very little bearing on the hardware present, it is mainly a matter for the software.</p>
<p class="baindent">The presence of three separate bus systems is of note and is largely due to the problems of interfacing with external hardware and maintaining compatibility with legacy devices. When the first IBM PCs were designed, the CPU operated at a clock rate of 4.75 MHz, and naturally all the rest of the circuitry followed this speed, including the external bus (called the ISA bus). Shortly thereafter the speed of the CPU increased to 8 MHz, and the ISA bus followed without too many problems (although some devices designed for the original machine did stop working). When the second generation of IBM <a id="page_21" class="page">Page 21, Chapter 2 The Hardware</a>PCs (the PC AT) were designed, they used an upgraded CPU (the 80286) which was capable of working at much higher speeds (up to 20 MHz); however it was not possible to reflect this clock speed in the ISA bus: the specifications of the bus stipulate a maximum clock speed of 8 MHz. Consequently, it was necessary to slow the whole machine down, whenever a device on the ISA bus was accessed. This slowing down was achieved by inserting<i> wait states</i> into the CPU clock &#x2013; effectively making the CPU miss one or two clock cycles while the ISA bus caught up. Unfortunately, as more and more programs relied on I/O for their operation (such as GUIs relying on access to the video systems), it was found that the CPU was spending much of its time in wait states. So, to alleviate this problem, it was necessary to have another bus system, in addition to the ISA bus, which allowed access to peripherals at the full speed of the processor. A number of proprietary systems were developed, the<i> local bus system</i> being the most widely used of the early versions. However there were a number of &#x2018;flavours&#x2019; of local bus system, and none became an industry standard. Eventually though, an existing industry standard, the 33 MHz PCI bus, was introduced into PCs and became accepted.</p>
<p class="baindent">This is not the end of the story for bus speeds. Many modern CPUs have stated speeds of 400 MHz or more. However, this does not mean that all the buses operate at these speeds. The speed stated is the internal bus speed of the chip,<i> i.e.</i> the speed at which the chip communicates between the ALU, registers, and, very importantly, the internal primary cache; when information is needed from an external source to the CPU, the bus speed is dropped to between 33 and 100 MHz. This means that there is still a need to provide interface circuitry between the host bus (running at, say 66 MHz) and the PCI bus (running at 33 MHz). Most PCs still have a number of ISA bus slots for slower peripherals: the ISA bus is now usually a &#x2018;device&#x2019; on the PCI bus. In the future there will undoubtedly be enhancements and modifications to this picture: already the PCI2 bus running at 66 MHz is being introduced into workstation class machines, and CPUs operating at 1 GHz are being developed.</p>
<p class="baindent">The bus each peripheral is attached to is largely dependent on the speed at which it operates: high bandwidth devices, such as video cards, disk controllers and high speed network cards, will be attached to the PCI bus, whereas slower devices, such as the serial and parallel ports, keyboard, mouse, audio and network cards will use the ISA bus. Most devices used for interfacing the PC to experiments will also be attached to the ISA bus as usually the rate of exchange of information is quite low.</p>
<h3 class="h3"><b>The Silicon Graphics Origin2000</b></h3>
<p class="banoindent">The Origin2000 is a multiple processor computer consisting of from 2 to 256 MIPS processors and up to 512 GB of memory. As would be expected, it is a very complex machine, and it will only be possible to scratch at its surface here. Nevertheless, it is instructive to examine some of the features of its design so that the basis of the advice on efficient programming given in <a href="Chapter05.xhtml">Chapter 5</a> will be more easily understood.</p>
<p class="baindent">The main feature of the system is the method by which groups of these processors intercommunicate. The basic system building block is a<i> node board</i>, each node board contains two processors and their associated cache <a id="page_22" class="page">Page 22, Chapter 2 The Hardware</a> (usually 4 MB), along with some main memory. The node board also contains a<i> hub</i> that controls the buses connecting the processors and memory. Each pair of node boards is then connected to a<i> router</i>; it is the router which controls how each of the node boards (and thus ultimately each processor) communicates in the system. All the routers in a system are interconnected <i>(via</i> the<i> CrayLink Interconnect</i>) in a manner that allows the most efficient interchange of data between the processors. A total system then consists of up to 16 modules, with each module containing 2 routers, 4 node boards and 8 processors.</p>
<figure class="image" id="fig2.5">
<img src="../images/img17.jpg" alt="images"/>
<figcaption>
<aside class="abc1" style="margin-top:-15em;" epub:type="sidebar">
Fig. 2.5 Block diagram of the Origin2000 node board (&#x00A9; see footnote on <a href="#page_24">page 24</a>)</aside></figcaption>
</figure>
<p class="baindent"><a href="#fig2.5">Figure 2.5</a> shows a simplified block diagram of a node board. The XIO port is the bus used to connect with peripherals such as disks or networks; it can be thought of as being equivalent to the I/O bus in PCs, although it is considerably faster. Each XIO system consists of an 8-port crossbow interconnect system, with 2 of the ports on the crossbow connected to two of the node boards in a module (either boards 1 &#x0026; 3 or 2 &#x0026; 4), with the other 6 ports being connected to individual XIO devices. The crossbow is simply a way of being able to connect any two ports on the device together: hence a node board can talk to any of the 6 XIO devices, and each of the XIO devices can talk to each other. <a href="#fig2.6">Fig. 2.6</a> shows the interconnections between the various components in a module.</p>
<figure class="image" id="fig2.6">
<img src="../images/img14.jpg" alt="images"/>
<figcaption>
<aside class="abc1" style="margin-top:-8em;" epub:type="sidebar">
Fig. 2.6 Block diagram of an Origin2000 module (&#x00A9; see footnote on <a href="#page_24">page 24</a>)</aside></figcaption>
</figure>
<p class="baindent">Each router board can connect up to six devices. Two of those devices are the node boards, and a further one is the other router board in the same <a id="page_23" class="page">Page 23, Chapter 2 The Hardware</a>module. This leaves three ports available for connection to other routers. In general, there are two modules in each rack, and so one of the external connections of each router is connected to the corresponding one in the other module in the rack. In this way, the individual routers in a rack are connected in a square and there are at least two routes between any two routers (<a href="#fig2.7">Fig. 2.7a</a>). When there is more than one rack, each of the routers within each rack is connected in this square topology, and then each router is connected to the corresponding one in the rack next to it, forming a cube arrangement (<a href="#fig2.7">Fig. 2.7b</a>). If there are 3 or 4 racks, then a similar style of expansion is made,<i> i.e.</i> each pair of racks is connected in the cube arrangement, then each vertex on the cube is connected to the corresponding one on the other cube, producing a &#x2018;hypercube&#x2019; topology (<a href="#fig2.7">Fig. 2.7c</a>). Finally, if there are more than 4 racks (or 64 CPUs), then each vertex of the cubes formed from each pair of racks (<i>i.e.</i> every router) is connected to a special device called a &#x2018;SGI Meta Router&#x2019; (<a href="#fig2.7">Fig. 2.7d</a>). This is a separate cabinet containing a network of routers and that allows all the node boards to talk to each other. Currently up to 32 modules (256 CPUs) can be interconnected using the Meta Router.</p>
<figure class="image" id="fig2.7" style="margin-left:-0.5em;">
<img src="../images/img15.jpg" alt="images"/>
<figcaption>
<aside class="abc1" style="margin-top:-23em;" epub:type="sidebar">
Fig. 2.7 Router connection topologies for the Silicon Graphics Origin2000.
<p class="noindent3" style="margin-left:2.4em;font-size:100%;width:100%;">(a)<span class="space">&#160;</span>8 node boards in a single rack (<i>i.e.</i> up to 16 CPUs)</p>
<p class="noindent3" style="margin-left:2.4em;font-size:100%;">(b)<span class="space">&#160;</span>16 node boards (up to 32 CPUs)</p>
<p class="noindent3" style="margin-left:2.4em;margin-bottom:0.5em;font-size:100%;">(c)<span class="space">&#160;</span>32 node boards.</p>
<p class="noindent2" style="margin-left:0.5em;font-size:100%;width:100%;">For clarity the individual CPUs are only shown in Fig 2.7a, each node board implicitly has two CPUs connected to it.<br/>(&#x00A9; see footnote on <a href="#page_24">page 24</a>)</p>
</aside>
</figcaption>
</figure>
<p class="baindent">In designing a large multiple processor system such as the Origin2000, many factors need to be taken into account. For instance, the fact that the main memory is distributed amongst all the node boards means that, unless precautions have been taken, the memory that a processor is accessing is likely to be on a different board to that processor, and no matter how fast the interconnections are, it will take longer to access that memory than if it were in the same location. A large processor cache tends to alleviate this problem (since with a large cache, the original memory locations need not be accessed as often), but it is still a factor that needs to be taken into account when the system software is being designed. A further, potentially more intractable problem is that of<i> cache coherency.</i> Cache coherency is making sure that the <a id="page_24" class="page">Page 24, Chapter 2 The Hardware</a>value that is currently held in a cache correctly represents the value that is in main memory. In a single processor system, this is no problem: in general, only the processor can change memory and so the value in the cache must be the same as in main memory. However, in a multiple processor system, any of the processors may alter a memory location, and so steps must be taken to ensure that when one processor does change a value, then all the other processors are told that any value they have in their cache corresponding to that location is now wrong. There are various schemes to ensure cache coherency; the brute force method is to clear the memory location from cache, whenever another processor writes to that location, thus forcing a reread the next time the memory is needed. The more subtle methods include broadcasting the new value to all the processors so that the correct value can be put in the cache. The Origin2000 uses a hybrid technique: the memory subsystem keeps a record of each processor that is using a particular memory location (this is the purpose of the<i> directory memory</i> in <a href="#fig2.5">Fig. 2.5</a>), when a processor needs to write to a location, it informs the memory subsystem which in turn tells all the processors that are using that location to invalidate that particular cache entry. When a value from that location is needed, the processor sends a request to the memory, which passes it on to the processor that invalidated the cache, this processor then sends back the correct value to both the processor that originated the request and the memory system. In this way the integrity of the cache is maintained without the expense of large numbers of messages passing between CPUs in order to maintain the coherency.</p>

<figure class="image">
<img src="../images/img20.jpg" alt="images"/>
<figcaption>
<aside class="abc1" style="margin-top:-15em;" epub:type="sidebar">
Fig. 2.7d The router interconnections in a 128 CPU Silicon Graphics Origin2000 system. Each cube of routers is connected to the others<i> via</i> the SGI Meta Router. (&#x00A9; see footnote)</aside></figcaption>
</figure>
<hr class="foot1" style="width:30%;"/>
<p class="foot">Figures <a href="#fig2.5">2.5</a>, <a href="#fig2.6">2.6</a>, <a href="#fig2.7">2.7a-d</a>: Illustrarion concept Courtesy of Silicon Graphics Inc.<br/>&#169; 1999 Silicon Graphics Inc. Used by permissinon. All Rights reserved.</p>
</section>
</section>
</body>
</html>